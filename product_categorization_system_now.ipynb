{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "337729b8",
      "metadata": {
        "id": "337729b8"
      },
      "source": [
        "# Smart Product Categorization System\n",
        "\n",
        "This notebook consolidates the entire project for training in Google Colab.\n",
        "\n",
        "**Categories:** `beverages`, `snacks`, `dry_food`, `non_food`\n",
        "\n",
        "**Architecture:** EfficientNet-B0 / ResNet-18 / MobileNet-V2 / SimpleCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3518279b",
      "metadata": {
        "id": "3518279b"
      },
      "source": [
        "## 0. Install Dependencies & Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de210e1c",
      "metadata": {
        "id": "de210e1c"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision scikit-learn matplotlib pandas Pillow tqdm huggingface-hub python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b21dbc",
      "metadata": {
        "id": "32b21dbc"
      },
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c729427",
      "metadata": {
        "id": "4c729427"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import random\n",
        "import tarfile\n",
        "import argparse\n",
        "import sys\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "    Callable, Dict, Iterable, List, Optional, Tuple, Union, Literal,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.models import (\n",
        "    EfficientNet_B0_Weights,\n",
        "    ResNet18_Weights,\n",
        "    MobileNet_V2_Weights,\n",
        ")\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import hf_hub_download\n",
        "except ImportError:\n",
        "    print(\"huggingface_hub not installed — manual data upload required.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e372bcd",
      "metadata": {
        "id": "7e372bcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98796aa9-02ce-4584-d60f-a18b9ebdd938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF_TOKEN set ✓\n"
          ]
        }
      ],
      "source": [
        "# ─── Set your Hugging Face token here ─────────────────────────────────────────\n",
        "# Option 1: paste directly\n",
        "HF_TOKEN = \"\"  # <-- paste your token or leave empty\n",
        "\n",
        "# Option 2: use Colab secrets\n",
        "if not HF_TOKEN:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if HF_TOKEN:\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    print(\"HF_TOKEN set ✓\")\n",
        "else:\n",
        "    print(\"⚠️  No HF_TOKEN found. Set it above or upload data manually.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c231480",
      "metadata": {
        "id": "3c231480"
      },
      "source": [
        "## 2. Data Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d8f1e0",
      "metadata": {
        "id": "48d8f1e0"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataConfig:\n",
        "    repo_id: str = \"Phathanan/product-categorization-system\"\n",
        "    repo_type: str = \"dataset\"\n",
        "    raw_tar_in_repo: str = \"data/raw/data_v2.tar\"\n",
        "    token: Optional[str] = None\n",
        "    revision: Optional[str] = None\n",
        "\n",
        "    dataset_dir: Path = Path(\"data_local\")\n",
        "    raw_extract_dirname: str = \"raw_extracted\"\n",
        "    processed_dirname: str = \"processed\"\n",
        "    raw_metadata_name: str = \"metadata.csv\"\n",
        "\n",
        "    labels: List[str] = field(\n",
        "        default_factory=lambda: [\"beverages\", \"snacks\", \"dry_food\", \"non_food\"]\n",
        "    )\n",
        "    dedup_by_barcode: bool = False\n",
        "    cap_per_label: Optional[int] = None\n",
        "\n",
        "    min_side: int = 128\n",
        "    do_verify: bool = False\n",
        "    num_workers: int = 8\n",
        "\n",
        "    seed: int = 42\n",
        "    train_frac: float = 0.70\n",
        "    val_frac: float = 0.15\n",
        "    test_frac: float = 0.15\n",
        "\n",
        "    def paths(self) -> dict:\n",
        "        tar_stem = Path(self.raw_tar_in_repo).name\n",
        "        if tar_stem.endswith(\".tar\"):\n",
        "            tar_stem = tar_stem[:-4]\n",
        "        else:\n",
        "            tar_stem = Path(tar_stem).stem\n",
        "\n",
        "        raw_dir = self.dataset_dir / self.raw_extract_dirname / tar_stem\n",
        "        proc_dir = self.dataset_dir / self.processed_dirname / tar_stem\n",
        "\n",
        "        return {\n",
        "            \"raw_dir\": raw_dir,\n",
        "            \"proc_dir\": proc_dir,\n",
        "            \"raw_metadata\": raw_dir / self.raw_metadata_name,\n",
        "            \"manifest_clean\": proc_dir / \"manifest_clean.csv\",\n",
        "            \"splits\": proc_dir / \"splits.json\",\n",
        "            \"stats\": proc_dir / \"stats.json\",\n",
        "            \"label_map\": proc_dir / \"label_map.json\",\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d168cc",
      "metadata": {
        "id": "e7d168cc"
      },
      "source": [
        "## 3. Train Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84473abe",
      "metadata": {
        "id": "84473abe"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrainConfig:\n",
        "    model_name: str = \"resnet18\"\n",
        "    freeze_backbone: bool = True\n",
        "    dropout: float = 0.5\n",
        "\n",
        "    manifest: Path = Path(\"data_local/processed/data_v2/manifest_clean.csv\")\n",
        "    label_map: Path = Path(\"data_local/processed/data_v2/label_map.json\")\n",
        "\n",
        "    epochs: int = 20\n",
        "    batch_size: int = 32\n",
        "    num_workers: int = 2\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-2\n",
        "\n",
        "    lr_scheduler: str = \"cosine\"\n",
        "    lr_step_size: int = 7\n",
        "    lr_gamma: float = 0.1\n",
        "\n",
        "    early_stop_patience: int = 3\n",
        "    early_stop_min_delta: float = 1e-4\n",
        "    early_stop_metric: str = \"val_loss\"\n",
        "\n",
        "    image_size: int = 224\n",
        "\n",
        "    output_dir: Path = Path(\"outputs\")\n",
        "\n",
        "    seed: int = 42\n",
        "    device: Optional[str] = None\n",
        "\n",
        "    def run_dir(self) -> Path:\n",
        "        d = self.output_dir / self.model_name\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "        return d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8913864",
      "metadata": {
        "id": "e8913864"
      },
      "source": [
        "## 4. Data Utilities — Loader, Prepare, Validate, Split, Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2133effd",
      "metadata": {
        "id": "2133effd"
      },
      "outputs": [],
      "source": [
        "# ─── loader.py ────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _extract_tar(tar_path: Path, out_dir: Path) -> None:\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    base = out_dir.resolve()\n",
        "    with tarfile.open(tar_path, \"r:*\") as tf:\n",
        "        members = tf.getmembers()\n",
        "        for m in members:\n",
        "            target = (out_dir / m.name).resolve()\n",
        "            if not str(target).startswith(str(base)):\n",
        "                raise RuntimeError(f\"Unsafe path in tar: {m.name}\")\n",
        "        tf.extractall(out_dir)\n",
        "\n",
        "\n",
        "def download_raw_tar(\n",
        "    repo_id: str,\n",
        "    path_in_repo: str,\n",
        "    repo_type: str = \"dataset\",\n",
        "    revision: Optional[str] = None,\n",
        "    token: Optional[str] = None,\n",
        ") -> Path:\n",
        "    return Path(\n",
        "        hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=path_in_repo,\n",
        "            repo_type=repo_type,\n",
        "            revision=revision,\n",
        "            token=token,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def ensure_extracted(raw_tar: Path, extract_dir: Path) -> None:\n",
        "    marker = extract_dir / \".extracted.ok\"\n",
        "    if marker.exists():\n",
        "        return\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "    _extract_tar(raw_tar, extract_dir)\n",
        "    marker.write_text(\"ok\", encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575d85f7",
      "metadata": {
        "id": "575d85f7"
      },
      "outputs": [],
      "source": [
        "# ─── prepare.py ───────────────────────────────────────────────────────────────\n",
        "\n",
        "def norm_barcode(x: object) -> str:\n",
        "    s = re.sub(r\"\\D\", \"\", str(x or \"\"))\n",
        "    return s.zfill(13) if s else \"\"\n",
        "\n",
        "\n",
        "def load_metadata(meta_path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(meta_path)\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_paths(df: pd.DataFrame, raw_dir: Path) -> pd.DataFrame:\n",
        "    raw_dir = Path(raw_dir)\n",
        "    images_dir = raw_dir / \"images\"\n",
        "    if not images_dir.exists():\n",
        "        images_dir = raw_dir\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"barcode\"] = df[\"barcode\"].map(norm_barcode)\n",
        "    df[\"image_id\"] = df[\"image_id\"].astype(\"string\").fillna(\"\").str.strip()\n",
        "\n",
        "    rel = (\n",
        "        df[\"image_id\"]\n",
        "        .str.replace(\"/\", os.sep, regex=False)\n",
        "        .str.lstrip(\"\\\\/\")\n",
        "    )\n",
        "\n",
        "    df[\"abs_path\"] = rel.map(lambda r: str(images_dir / r))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def basic_clean(\n",
        "    df: pd.DataFrame,\n",
        "    labels: Optional[list] = None,\n",
        "    dedup_by_barcode: bool = True,\n",
        "    cap_per_label: Optional[int] = None,\n",
        "    seed: int = 42,\n",
        ") -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    if \"label_coarse\" not in df.columns:\n",
        "        raise ValueError(\"metadata must contain label_coarse\")\n",
        "\n",
        "    df[\"label_coarse\"] = df[\"label_coarse\"].astype(str).str.strip()\n",
        "    df = df[df[\"barcode\"].astype(str).str.len() > 0]\n",
        "    df = df[df[\"image_id\"].astype(str).str.len() > 0]\n",
        "    df = df[df[\"abs_path\"].astype(str).str.len() > 0]\n",
        "\n",
        "    if labels:\n",
        "        df = df[df[\"label_coarse\"].isin(labels)]\n",
        "\n",
        "    df = df.drop_duplicates(subset=[\"abs_path\"], keep=\"first\")\n",
        "\n",
        "    if dedup_by_barcode:\n",
        "        df = df.sort_values([\"barcode\", \"label_coarse\", \"image_id\"])\n",
        "        df = df.drop_duplicates(subset=[\"barcode\"], keep=\"first\")\n",
        "\n",
        "    if cap_per_label is not None:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        kept = []\n",
        "        for lbl, g in df.groupby(\"label_coarse\"):\n",
        "            if len(g) <= cap_per_label:\n",
        "                kept.append(g)\n",
        "            else:\n",
        "                idx = rng.choice(g.index.to_numpy(), size=cap_per_label, replace=False)\n",
        "                kept.append(df.loc[idx])\n",
        "        df = pd.concat(kept, ignore_index=True)\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def attach_label_map(labels: list) -> dict:\n",
        "    return {lbl: i for i, lbl in enumerate(labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885f478a",
      "metadata": {
        "id": "885f478a"
      },
      "outputs": [],
      "source": [
        "# ─── validate.py ──────────────────────────────────────────────────────────────\n",
        "\n",
        "def _check_one(path: str, min_side: int, do_verify: bool) -> Tuple[int, int, int, int]:\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        return 0, 0, 0, 0\n",
        "    try:\n",
        "        if do_verify:\n",
        "            with Image.open(p) as im:\n",
        "                im.verify()\n",
        "        with Image.open(p) as im:\n",
        "            w, h = im.size\n",
        "        ok = int(min(w, h) >= min_side)\n",
        "        size = int(p.stat().st_size)\n",
        "        return ok, w, h, size\n",
        "    except Exception:\n",
        "        return 0, 0, 0, 0\n",
        "\n",
        "\n",
        "def validate_images(\n",
        "    df: pd.DataFrame,\n",
        "    min_side: int = 128,\n",
        "    do_verify: bool = False,\n",
        "    num_workers: int = 8,\n",
        ") -> pd.DataFrame:\n",
        "    if \"abs_path\" not in df.columns:\n",
        "        raise ValueError(\"df must contain abs_path\")\n",
        "\n",
        "    paths = df[\"abs_path\"].astype(str).tolist()\n",
        "    results = [None] * len(paths)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max(1, int(num_workers))) as ex:\n",
        "        futs = {\n",
        "            ex.submit(_check_one, paths[i], min_side, do_verify): i\n",
        "            for i in range(len(paths))\n",
        "        }\n",
        "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"validate images\"):\n",
        "            i = futs[fut]\n",
        "            results[i] = fut.result()\n",
        "\n",
        "    out = df.copy()\n",
        "    out[\"img_ok\"] = [r[0] for r in results]\n",
        "    out[\"w\"] = [r[1] for r in results]\n",
        "    out[\"h\"] = [r[2] for r in results]\n",
        "    out[\"file_size\"] = [r[3] for r in results]\n",
        "    return out\n",
        "\n",
        "\n",
        "def keep_only_ok(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"img_ok\" not in df.columns:\n",
        "        raise ValueError(\"df must contain img_ok\")\n",
        "    return df[df[\"img_ok\"] == 1].copy().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2f1f78",
      "metadata": {
        "id": "cc2f1f78"
      },
      "outputs": [],
      "source": [
        "# ─── split.py ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "@dataclass\n",
        "class SplitConfig:\n",
        "    seed: int = 42\n",
        "    train_frac: float = 0.8\n",
        "    val_frac: float = 0.1\n",
        "    test_frac: float = 0.1\n",
        "\n",
        "\n",
        "def _alloc_counts(n: int, train_f: float, val_f: float, test_f: float) -> Tuple[int, int, int]:\n",
        "    if n <= 0:\n",
        "        return 0, 0, 0\n",
        "    if n == 1:\n",
        "        return 1, 0, 0\n",
        "    if n == 2:\n",
        "        return 1, 1, 0\n",
        "    val = max(1, int(round(n * val_f)))\n",
        "    test = max(1, int(round(n * test_f)))\n",
        "    if val + test >= n:\n",
        "        val = 1\n",
        "        test = 1\n",
        "    train = n - val - test\n",
        "    if train <= 0:\n",
        "        train = max(1, n - 2)\n",
        "        val = 1 if n - train >= 1 else 0\n",
        "        test = n - train - val\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def split_by_barcode(df: pd.DataFrame, cfg: SplitConfig) -> Tuple[pd.DataFrame, Dict]:\n",
        "    if \"barcode\" not in df.columns or \"label_coarse\" not in df.columns:\n",
        "        raise ValueError(\"df must contain barcode and label_coarse\")\n",
        "\n",
        "    rng = np.random.default_rng(cfg.seed)\n",
        "    pairs = df[[\"barcode\", \"label_coarse\"]].drop_duplicates()\n",
        "    barcode_to_label = dict(zip(pairs[\"barcode\"], pairs[\"label_coarse\"]))\n",
        "\n",
        "    split_map: Dict[str, str] = {}\n",
        "\n",
        "    for lbl, g in pairs.groupby(\"label_coarse\"):\n",
        "        barcodes = g[\"barcode\"].tolist()\n",
        "        rng.shuffle(barcodes)\n",
        "\n",
        "        n = len(barcodes)\n",
        "        n_train, n_val, n_test = _alloc_counts(n, cfg.train_frac, cfg.val_frac, cfg.test_frac)\n",
        "\n",
        "        train_ids = barcodes[:n_train]\n",
        "        val_ids = barcodes[n_train: n_train + n_val]\n",
        "        test_ids = barcodes[n_train + n_val: n_train + n_val + n_test]\n",
        "\n",
        "        for b in train_ids:\n",
        "            split_map[b] = \"train\"\n",
        "        for b in val_ids:\n",
        "            split_map[b] = \"val\"\n",
        "        for b in test_ids:\n",
        "            split_map[b] = \"test\"\n",
        "\n",
        "    out = df.copy()\n",
        "    out[\"split\"] = out[\"barcode\"].map(split_map).fillna(\"train\")\n",
        "\n",
        "    splits = {\"train\": [], \"val\": [], \"test\": []}\n",
        "    for b, s in split_map.items():\n",
        "        splits[s].append(b)\n",
        "\n",
        "    meta = {\n",
        "        \"seed\": cfg.seed,\n",
        "        \"fractions\": {\n",
        "            \"train\": cfg.train_frac,\n",
        "            \"val\": cfg.val_frac,\n",
        "            \"test\": cfg.test_frac,\n",
        "        },\n",
        "        \"counts\": {k: len(v) for k, v in splits.items()},\n",
        "        \"splits\": splits,\n",
        "        \"barcode_label\": barcode_to_label,\n",
        "    }\n",
        "    return out, meta\n",
        "\n",
        "\n",
        "def save_splits_json(meta: Dict, out_path: Path) -> None:\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_path.write_text(\n",
        "        json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49774e19",
      "metadata": {
        "id": "49774e19"
      },
      "outputs": [],
      "source": [
        "# ─── stats.py ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "def compute_stats(df: pd.DataFrame) -> Dict:\n",
        "    total = int(len(df))\n",
        "    by_label = (\n",
        "        df[\"label_coarse\"].value_counts().to_dict()\n",
        "        if \"label_coarse\" in df.columns\n",
        "        else {}\n",
        "    )\n",
        "    by_split = (\n",
        "        df[\"split\"].value_counts().to_dict() if \"split\" in df.columns else {}\n",
        "    )\n",
        "\n",
        "    by_label_split = {}\n",
        "    if \"label_coarse\" in df.columns and \"split\" in df.columns:\n",
        "        tmp = df.groupby([\"label_coarse\", \"split\"]).size().reset_index(name=\"n\")\n",
        "        for _, r in tmp.iterrows():\n",
        "            lbl = r[\"label_coarse\"]\n",
        "            if lbl not in by_label_split:\n",
        "                by_label_split[lbl] = {}\n",
        "            by_label_split[lbl][r[\"split\"]] = int(r[\"n\"])\n",
        "\n",
        "    img_ok_rate = None\n",
        "    if \"img_ok\" in df.columns:\n",
        "        img_ok_rate = float(df[\"img_ok\"].mean()) if len(df) else 0.0\n",
        "\n",
        "    return {\n",
        "        \"total\": total,\n",
        "        \"by_label\": {k: int(v) for k, v in by_label.items()},\n",
        "        \"by_split\": {k: int(v) for k, v in by_split.items()},\n",
        "        \"by_label_split\": by_label_split,\n",
        "        \"img_ok_rate\": img_ok_rate,\n",
        "    }\n",
        "\n",
        "\n",
        "def save_stats(stats: Dict, out_path: Path) -> None:\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_path.write_text(\n",
        "        json.dumps(stats, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a82881c5",
      "metadata": {
        "id": "a82881c5"
      },
      "source": [
        "## 5. Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ec83b4",
      "metadata": {
        "id": "64ec83b4"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "def get_train_transforms(size: int = 224) -> transforms.Compose:\n",
        "    return transforms.Compose(\n",
        "        [\n",
        "            # 1. ปรับขนาดภาพเบื้องต้น\n",
        "            transforms.Resize(size + 32),\n",
        "            transforms.RandomResizedCrop(\n",
        "                size,\n",
        "                scale=(0.7, 1.0),\n",
        "                ratio=(0.75, 1.33),\n",
        "                interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "            ),\n",
        "\n",
        "            # 2. TrivialAugmentWide: สุ่มเลือกชุด Augmentation อัตโนมัติ (มีประสิทธิภาพสูงมาก)\n",
        "            transforms.TrivialAugmentWide(),\n",
        "\n",
        "            # 3. RandomAffine: เพิ่มความทนทานต่อการเอียง (degrees), การเลื่อน (translate),\n",
        "            # การย่อขยาย (scale) และการบิดเบี้ยว (shear)\n",
        "            transforms.RandomAffine(\n",
        "                degrees=20,\n",
        "                translate=(0.1, 0.1),\n",
        "                scale=(0.9, 1.1),\n",
        "                shear=10\n",
        "            ),\n",
        "\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "            # 4. Gaussian Blur: สุ่มทำภาพเบลอ (สุ่มใช้ที่ความน่าจะเป็น 30%)\n",
        "            transforms.RandomApply([\n",
        "                transforms.GaussianBlur(kernel_size=(3, 7), sigma=(0.1, 2.0))\n",
        "            ], p=0.3),\n",
        "\n",
        "            # 5. แปลงเป็น Tensor และ Normalize\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def get_val_transforms(size: int = 224) -> transforms.Compose:\n",
        "    return transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1abab676",
      "metadata": {
        "id": "1abab676"
      },
      "source": [
        "## 6. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b4dea02",
      "metadata": {
        "id": "8b4dea02"
      },
      "outputs": [],
      "source": [
        "class ProductDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for product package image classification.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    manifest : pd.DataFrame | Path | str\n",
        "        Either a pre-loaded DataFrame or a path to manifest_clean.csv.\n",
        "        The DataFrame must contain columns: ``abs_path``, ``label_coarse``.\n",
        "    label_map : Dict[str, int] | Path | str\n",
        "        Either a pre-built {class_name: int} dict or a path to label_map.json.\n",
        "    split : str | None\n",
        "        If not None, filter the manifest to rows where ``split == split``.\n",
        "        Typical values: ``\"train\"``, ``\"val\"``, ``\"test\"``.\n",
        "    transform : Callable | None\n",
        "        torchvision transform pipeline applied to the PIL image.\n",
        "        Use ``get_train_transforms()`` / ``get_val_transforms()`` from transforms.py.\n",
        "\n",
        "    Returns  (via __getitem__)\n",
        "    -------\n",
        "    image  : torch.FloatTensor  shape (C, H, W) after transform\n",
        "    label  : torch.LongTensor   scalar integer class index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        manifest: Union[pd.DataFrame, Path, str],\n",
        "        label_map: Union[Dict[str, int], Path, str],\n",
        "        split: Optional[str] = None,\n",
        "        transform: Optional[Callable] = None,\n",
        "    ) -> None:\n",
        "        # ── 1. Load manifest ──────────────────────────────────────────────\n",
        "        manifest_path = manifest if isinstance(manifest, (str, Path)) else None\n",
        "        if isinstance(manifest, (str, Path)):\n",
        "            manifest = pd.read_csv(manifest)\n",
        "\n",
        "        if \"abs_path\" not in manifest.columns:\n",
        "            raise ValueError(\"manifest must contain column 'abs_path'\")\n",
        "        if \"label_coarse\" not in manifest.columns:\n",
        "            raise ValueError(\"manifest must contain column 'label_coarse'\")\n",
        "\n",
        "        # ── 2. Filter by split ────────────────────────────────────────────\n",
        "        if split is not None:\n",
        "            if \"split\" not in manifest.columns:\n",
        "                if manifest_path is not None:\n",
        "                    splits_path = Path(manifest_path).parent / \"splits.json\"\n",
        "                    if splits_path.exists():\n",
        "                        splits_data = json.loads(splits_path.read_text(encoding=\"utf-8\"))\n",
        "                        barcodes = set(splits_data.get(\"splits\", {}).get(split, []))\n",
        "\n",
        "                        # We need 'barcode' as a string without '.0' etc\n",
        "                        # Pandas sometimes reads big numbers as float if there are NaNs\n",
        "                        manifest[\"_tmp_bc\"] = manifest.get(\"barcode\", \"\").astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "                        manifest = manifest[manifest[\"_tmp_bc\"].isin(barcodes)].copy()\n",
        "                        manifest = manifest.drop(columns=[\"_tmp_bc\"])\n",
        "                    else:\n",
        "                        raise ValueError(f\"split '{split}' requested, no 'split' column, and no splits.json found at {splits_path}\")\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        f\"split='{split}' requested but manifest has no 'split' column. \"\n",
        "                        \"Run scripts/prepare_dataset.py first.\"\n",
        "                    )\n",
        "            else:\n",
        "                manifest = manifest[manifest[\"split\"] == split].copy()\n",
        "\n",
        "            if len(manifest) == 0:\n",
        "                raise ValueError(\n",
        "                    f\"No rows found for split='{split}'. \"\n",
        "                    \"Check that prepare_dataset.py completed successfully.\"\n",
        "                )\n",
        "\n",
        "        self._df = manifest.reset_index(drop=True)\n",
        "\n",
        "        # ── 3. Load label_map ─────────────────────────────────────────────\n",
        "        if isinstance(label_map, (str, Path)):\n",
        "            label_map = json.loads(Path(label_map).read_text(encoding=\"utf-8\"))\n",
        "\n",
        "        self._label_map: Dict[str, int] = label_map\n",
        "\n",
        "        # Pre-validate that every label in manifest is known\n",
        "        unknown = set(self._df[\"label_coarse\"].unique()) - set(self._label_map.keys())\n",
        "        if unknown:\n",
        "            raise ValueError(\n",
        "                f\"Labels found in manifest but missing from label_map: {unknown}\"\n",
        "            )\n",
        "\n",
        "        self.transform = transform\n",
        "        self.classes: list = sorted(self._label_map, key=self._label_map.get)  # type: ignore[arg-type]\n",
        "        self.num_classes: int = len(self._label_map)\n",
        "\n",
        "    # ── Dataset protocol ──────────────────────────────────────────────────\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._df)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        row = self._df.iloc[idx]\n",
        "\n",
        "        # ── Load image (PIL, RGB) ────────────────────────────────────────\n",
        "        img_path = Path(str(row[\"abs_path\"]))\n",
        "        try:\n",
        "            image: Image.Image = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception as exc:\n",
        "            raise RuntimeError(\n",
        "                f\"Cannot open image at index {idx}: {img_path}\"\n",
        "            ) from exc\n",
        "\n",
        "        # ── Apply transform pipeline ─────────────────────────────────────\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Fallback: at minimum convert PIL → tensor if no transform supplied\n",
        "            from torchvision.transforms.functional import to_tensor\n",
        "            image = to_tensor(image)  # type: ignore[assignment]\n",
        "\n",
        "        # ── Encode label ─────────────────────────────────────────────────\n",
        "        label_int: int = self._label_map[row[\"label_coarse\"]]\n",
        "        label = torch.tensor(label_int, dtype=torch.long)\n",
        "\n",
        "        return image, label  # type: ignore[return-value]\n",
        "\n",
        "    # ── Convenience helpers ───────────────────────────────────────────────\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"ProductDataset(\"\n",
        "            f\"n={len(self)}, \"\n",
        "            f\"split={self._df['split'].unique().tolist() if 'split' in self._df.columns else 'N/A'}, \"\n",
        "            f\"classes={self.classes}\"\n",
        "            f\")\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def label_map(self) -> Dict[str, int]:\n",
        "        return dict(self._label_map)\n",
        "\n",
        "\n",
        "# ── Convenience factory ───────────────────────────────────────────────────────\n",
        "\n",
        "def build_datasets(\n",
        "    manifest_path: Union[Path, str],\n",
        "    label_map_path: Union[Path, str],\n",
        "    train_transform: Optional[Callable] = None,\n",
        "    val_transform: Optional[Callable] = None,\n",
        ") -> Dict[str, \"ProductDataset\"]:\n",
        "    \"\"\"\n",
        "    Build train / val / test datasets in one call.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from src.data.transforms import get_train_transforms, get_val_transforms\n",
        "    >>> from src.config.data_config import DataConfig\n",
        "    >>> cfg = DataConfig()\n",
        "    >>> p = cfg.paths()\n",
        "    >>> datasets = build_datasets(\n",
        "    ...     p[\"manifest_clean\"], p[\"label_map\"],\n",
        "    ...     train_transform=get_train_transforms(),\n",
        "    ...     val_transform=get_val_transforms(),\n",
        "    ... )\n",
        "    >>> datasets[\"train\"], datasets[\"val\"], datasets[\"test\"]\n",
        "    \"\"\"\n",
        "    manifest = pd.read_csv(manifest_path)\n",
        "\n",
        "    if \"split\" not in manifest.columns:\n",
        "        splits_path = Path(manifest_path).parent / \"splits.json\"\n",
        "        if splits_path.exists():\n",
        "            splits_data = json.loads(splits_path.read_text(encoding=\"utf-8\"))\n",
        "            barcode_to_split = {}\n",
        "            for sp, bcs in splits_data.get(\"splits\", {}).items():\n",
        "                for bc in bcs:\n",
        "                    barcode_to_split[str(bc)] = sp\n",
        "\n",
        "            manifest[\"_tmp_bc\"] = manifest.get(\"barcode\", \"\").astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "            manifest[\"split\"] = manifest[\"_tmp_bc\"].map(barcode_to_split)\n",
        "            manifest = manifest.drop(columns=[\"_tmp_bc\"])\n",
        "            manifest = manifest.dropna(subset=[\"split\"])\n",
        "\n",
        "    label_map: Dict[str, int] = json.loads(\n",
        "        Path(label_map_path).read_text(encoding=\"utf-8\")\n",
        "    )\n",
        "\n",
        "    ds: Dict[str, ProductDataset] = {}\n",
        "    for split_name in (\"train\", \"val\", \"test\"):\n",
        "        transform = train_transform if split_name == \"train\" else val_transform\n",
        "        ds[split_name] = ProductDataset(\n",
        "            manifest=manifest,\n",
        "            label_map=label_map,\n",
        "            split=split_name,\n",
        "            transform=transform,\n",
        "        )\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec5a13da",
      "metadata": {
        "id": "ec5a13da"
      },
      "source": [
        "## 7. Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fe2af8",
      "metadata": {
        "id": "91fe2af8"
      },
      "outputs": [],
      "source": [
        "# ─── ProductClassifier (EfficientNet-B0) ──────────────────────────────────────\n",
        "\n",
        "class ProductClassifier(nn.Module):\n",
        "    BACKBONE_OUT_FEATURES: int = 1280\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 4,\n",
        "        freeze_backbone: bool = True,\n",
        "        dropout: float = 0.3,\n",
        "        pretrained: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        backbone = models.efficientnet_b0(weights=weights)\n",
        "\n",
        "        self.backbone: nn.Module = backbone.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(self.BACKBONE_OUT_FEATURES, num_classes),\n",
        "        )\n",
        "\n",
        "        nn.init.xavier_uniform_(self.head[2].weight)\n",
        "        nn.init.zeros_(self.head[2].bias)\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self._backbone_frozen = False\n",
        "\n",
        "        if freeze_backbone:\n",
        "            self.freeze_backbone()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.backbone(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "    def freeze_backbone(self) -> None:\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        self._backbone_frozen = True\n",
        "\n",
        "    def unfreeze_backbone(self) -> None:\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = True\n",
        "        self._backbone_frozen = False\n",
        "\n",
        "    def unfreeze_last_n_blocks(self, n: int = 3) -> None:\n",
        "        blocks = list(self.backbone.children())\n",
        "        for block in blocks[-n:]:\n",
        "            for param in block.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def trainable_params(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def total_params(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def param_summary(self) -> Dict[str, int]:\n",
        "        total = self.total_params()\n",
        "        trainable = self.trainable_params()\n",
        "        return {\"total\": total, \"trainable\": trainable, \"frozen\": total - trainable}\n",
        "\n",
        "    def save(self, path: Path) -> None:\n",
        "        path = Path(path)\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model_state_dict\": self.state_dict(),\n",
        "                \"num_classes\": self.num_classes,\n",
        "            },\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: Path, num_classes: Optional[int] = None, map_location: str = \"cpu\") -> \"ProductClassifier\":\n",
        "        ckpt = torch.load(path, map_location=map_location)\n",
        "        nc = num_classes or ckpt[\"num_classes\"]\n",
        "        model = cls(num_classes=nc, freeze_backbone=False, pretrained=False)\n",
        "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "        return model\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"ProductClassifier(\"\n",
        "            f\"backbone=EfficientNet-B0, \"\n",
        "            f\"num_classes={self.num_classes}, \"\n",
        "            f\"frozen={self._backbone_frozen}, \"\n",
        "            f\"trainable_params={self.trainable_params():,})\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c63f7c",
      "metadata": {
        "id": "76c63f7c"
      },
      "outputs": [],
      "source": [
        "# ─── SimpleCNN ────────────────────────────────────────────────────────────────\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int = 4, dropout: float = 0.3) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self) -> None:\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def freeze_backbone(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def unfreeze_backbone(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def trainable_params(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def total_params(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def param_summary(self) -> dict:\n",
        "        total = self.total_params()\n",
        "        trainable = self.trainable_params()\n",
        "        return {\"total\": total, \"trainable\": trainable, \"frozen\": total - trainable}\n",
        "\n",
        "    def save(self, path) -> None:\n",
        "        path = Path(path)\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        torch.save(\n",
        "            {\"model_state_dict\": self.state_dict(), \"num_classes\": self.num_classes},\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"SimpleCNN(num_classes={self.num_classes}, trainable_params={self.trainable_params():,})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db7f085",
      "metadata": {
        "id": "6db7f085"
      },
      "outputs": [],
      "source": [
        "# ─── TransferModel wrapper (for ResNet-18 / MobileNet-V2) ────────────────────\n",
        "\n",
        "class _TransferModel(nn.Module):\n",
        "    def __init__(self, backbone: nn.Module, num_classes: int, freeze_backbone: bool) -> None:\n",
        "        super().__init__()\n",
        "        self._backbone = backbone\n",
        "        self.num_classes = num_classes\n",
        "        self._backbone_frozen = False\n",
        "        if freeze_backbone:\n",
        "            self.freeze_backbone()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self._backbone(x)\n",
        "\n",
        "    def freeze_backbone(self) -> None:\n",
        "        for name, param in self._backbone.named_parameters():\n",
        "            if not name.startswith(\"fc.\") and not name.startswith(\"classifier.\"):\n",
        "                param.requires_grad = False\n",
        "        self._backbone_frozen = True\n",
        "\n",
        "    def unfreeze_backbone(self) -> None:\n",
        "        for param in self._backbone.parameters():\n",
        "            param.requires_grad = True\n",
        "        self._backbone_frozen = False\n",
        "\n",
        "    def trainable_params(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def total_params(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def param_summary(self) -> dict:\n",
        "        total = self.total_params()\n",
        "        trainable = self.trainable_params()\n",
        "        return {\"total\": total, \"trainable\": trainable, \"frozen\": total - trainable}\n",
        "\n",
        "    def save(self, path) -> None:\n",
        "        path = Path(path)\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        torch.save(\n",
        "            {\"model_state_dict\": self.state_dict(), \"num_classes\": self.num_classes},\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        name = type(self._backbone).__name__\n",
        "        return (\n",
        "            f\"{name}Wrapper(\"\n",
        "            f\"num_classes={self.num_classes}, \"\n",
        "            f\"frozen={self._backbone_frozen}, \"\n",
        "            f\"trainable_params={self.trainable_params():,})\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7309f2",
      "metadata": {
        "id": "8c7309f2"
      },
      "outputs": [],
      "source": [
        "# ─── Model factory ────────────────────────────────────────────────────────────\n",
        "\n",
        "def _build_resnet18(num_classes: int, freeze_backbone: bool, dropout: float) -> _TransferModel:\n",
        "    backbone = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    in_features = backbone.fc.in_features\n",
        "    backbone.fc = nn.Sequential(\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features, num_classes),\n",
        "    )\n",
        "    nn.init.xavier_uniform_(backbone.fc[1].weight)\n",
        "    nn.init.zeros_(backbone.fc[1].bias)\n",
        "    return _TransferModel(backbone, num_classes, freeze_backbone)\n",
        "\n",
        "\n",
        "def _build_mobilenetv2(num_classes: int, freeze_backbone: bool, dropout: float) -> _TransferModel:\n",
        "    backbone = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "    in_features = backbone.classifier[1].in_features\n",
        "    backbone.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features, num_classes),\n",
        "    )\n",
        "    nn.init.xavier_uniform_(backbone.classifier[1].weight)\n",
        "    nn.init.zeros_(backbone.classifier[1].bias)\n",
        "    return _TransferModel(backbone, num_classes, freeze_backbone)\n",
        "\n",
        "def _build_resnet50(num_classes: int, freeze_backbone: bool, dropout: float) -> _TransferModel:\n",
        "    backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    in_features = backbone.fc.in_features\n",
        "    backbone.fc = nn.Sequential(\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features, num_classes),\n",
        "    )\n",
        "    nn.init.xavier_uniform_(backbone.fc[1].weight)\n",
        "    nn.init.zeros_(backbone.fc[1].bias)\n",
        "    return _TransferModel(backbone, num_classes, freeze_backbone)\n",
        "\n",
        "\n",
        "def _build_mobilenetv3_large(num_classes: int, freeze_backbone: bool, dropout: float) -> _TransferModel:\n",
        "    backbone = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2)\n",
        "    in_features = backbone.classifier[-1].in_features\n",
        "    backbone.classifier[-1] = nn.Sequential(\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features, num_classes),\n",
        "    )\n",
        "    # classifier[-1] เป็น Sequential ใหม่ -> weight อยู่ที่ [1]\n",
        "    nn.init.xavier_uniform_(backbone.classifier[-1][1].weight)\n",
        "    nn.init.zeros_(backbone.classifier[-1][1].bias)\n",
        "    return _TransferModel(backbone, num_classes, freeze_backbone)\n",
        "\n",
        "\n",
        "_REGISTRY = {\n",
        "    \"efficientnet_b0\": lambda nc, fb, do: ProductClassifier(num_classes=nc, freeze_backbone=fb, dropout=do),\n",
        "    \"simple_cnn\": lambda nc, fb, do: SimpleCNN(num_classes=nc, dropout=do),\n",
        "    \"resnet18\": _build_resnet18,\n",
        "    \"resnet50\": _build_resnet50,\n",
        "    \"mobilenetv2\": _build_mobilenetv2,\n",
        "    \"mobilenetv3_large\": _build_mobilenetv3_large,\n",
        "}\n",
        "\n",
        "ModelName = Literal[\"efficientnet_b0\", \"simple_cnn\", \"resnet18\", \"mobilenetv2\"]\n",
        "\n",
        "\n",
        "def build_model(\n",
        "    name: str,\n",
        "    num_classes: int = 4,\n",
        "    freeze_backbone: bool = True,\n",
        "    dropout: float = 0.3,\n",
        ") -> nn.Module:\n",
        "    name = name.lower().strip()\n",
        "    if name not in _REGISTRY:\n",
        "        raise ValueError(\n",
        "            f\"Unknown model '{name}'. Choose from: {list(_REGISTRY.keys())}\"\n",
        "        )\n",
        "    return _REGISTRY[name](num_classes, freeze_backbone, dropout)\n",
        "\n",
        "\n",
        "def available_models() -> list:\n",
        "    return list(_REGISTRY.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9868372d",
      "metadata": {
        "id": "9868372d"
      },
      "source": [
        "## 8. Metrics & Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d34af32",
      "metadata": {
        "id": "0d34af32"
      },
      "outputs": [],
      "source": [
        "# ─── metrics.py ───────────────────────────────────────────────────────────────\n",
        "\n",
        "def compute_metrics(\n",
        "    all_labels: List[int],\n",
        "    all_preds: List[int],\n",
        "    class_names: Optional[List[str]] = None,\n",
        ") -> Dict[str, float]:\n",
        "    y_true = np.array(all_labels)\n",
        "    y_pred = np.array(all_preds)\n",
        "\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    labels_arg = list(range(len(class_names))) if class_names is not None else None\n",
        "    f1_macro = float(f1_score(y_true, y_pred, labels=labels_arg, average=\"macro\", zero_division=0))\n",
        "\n",
        "    result: Dict[str, float] = {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1_macro\": f1_macro,\n",
        "    }\n",
        "\n",
        "    if class_names:\n",
        "        per_class = f1_score(y_true, y_pred, labels=labels_arg, average=None, zero_division=0)\n",
        "        for name, score in zip(class_names, per_class):\n",
        "            result[f\"f1_{name}\"] = float(score)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_classification_report(\n",
        "    all_labels: List[int],\n",
        "    all_preds: List[int],\n",
        "    class_names: Optional[List[str]] = None,\n",
        ") -> str:\n",
        "    labels_arg = list(range(len(class_names))) if class_names is not None else None\n",
        "    return classification_report(\n",
        "        all_labels, all_preds, labels=labels_arg, target_names=class_names, zero_division=0\n",
        "    )\n",
        "\n",
        "\n",
        "def get_confusion_matrix(\n",
        "    all_labels: List[int],\n",
        "    all_preds: List[int],\n",
        "    class_names: Optional[List[str]] = None,\n",
        ") -> np.ndarray:\n",
        "    labels_arg = list(range(len(class_names))) if class_names is not None else None\n",
        "    return confusion_matrix(all_labels, all_preds, labels=labels_arg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7279d21",
      "metadata": {
        "id": "a7279d21"
      },
      "outputs": [],
      "source": [
        "# ─── logger.py ────────────────────────────────────────────────────────────────\n",
        "\n",
        "class CSVLogger:\n",
        "    def __init__(self, path: Path) -> None:\n",
        "        self.path = Path(path)\n",
        "        self._header_written = self.path.exists()\n",
        "\n",
        "    def log(self, epoch: int, split: str, **metrics: float) -> None:\n",
        "        row = {\"epoch\": epoch, \"split\": split, **metrics}\n",
        "        write_header = not self._header_written\n",
        "        with self.path.open(\"a\", newline=\"\") as fh:\n",
        "            writer = csv.DictWriter(fh, fieldnames=list(row.keys()))\n",
        "            if write_header:\n",
        "                writer.writeheader()\n",
        "                self._header_written = True\n",
        "            writer.writerow(row)\n",
        "\n",
        "\n",
        "def _read_csv(path: Path) -> List[Dict]:\n",
        "    with path.open() as fh:\n",
        "        return list(csv.DictReader(fh))\n",
        "\n",
        "\n",
        "def plot_loss_curves(csv_path: Path, out_path: Path) -> None:\n",
        "    try:\n",
        "        import matplotlib\n",
        "        matplotlib.use(\"Agg\")\n",
        "        import matplotlib.pyplot as plt\n",
        "    except ImportError:\n",
        "        print(\"[logger] matplotlib not installed — skipping loss curve plot.\")\n",
        "        return\n",
        "\n",
        "    rows = _read_csv(csv_path)\n",
        "    train_rows = [r for r in rows if r[\"split\"] == \"train\"]\n",
        "    val_rows = [r for r in rows if r[\"split\"] == \"val\"]\n",
        "\n",
        "    if not train_rows or not val_rows:\n",
        "        return\n",
        "\n",
        "    epochs = [int(r[\"epoch\"]) for r in train_rows]\n",
        "    train_loss = [float(r[\"loss\"]) for r in train_rows]\n",
        "    val_loss = [float(r[\"loss\"]) for r in val_rows]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(epochs, train_loss, label=\"train_loss\", marker=\"o\", markersize=3)\n",
        "    ax.plot(epochs, val_loss, label=\"val_loss\", marker=\"s\", markersize=3)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.set_title(\"Train vs Val Loss\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=120)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_accuracy_curve(csv_path: Path, out_path: Path) -> None:\n",
        "    try:\n",
        "        import matplotlib\n",
        "        matplotlib.use(\"Agg\")\n",
        "        import matplotlib.pyplot as plt\n",
        "    except ImportError:\n",
        "        print(\"[logger] matplotlib not installed — skipping accuracy curve plot.\")\n",
        "        return\n",
        "\n",
        "    rows = _read_csv(csv_path)\n",
        "    val_rows = [r for r in rows if r[\"split\"] == \"val\"]\n",
        "\n",
        "    if not val_rows:\n",
        "        return\n",
        "\n",
        "    epochs = [int(r[\"epoch\"]) for r in val_rows]\n",
        "    accuracy = [float(r[\"accuracy\"]) for r in val_rows]\n",
        "    f1 = [float(r[\"f1_macro\"]) for r in val_rows]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(epochs, accuracy, label=\"val_accuracy\", marker=\"o\", markersize=3)\n",
        "    ax.plot(epochs, f1, label=\"val_f1_macro\", marker=\"s\", markersize=3)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_ylim(0, 1.05)\n",
        "    ax.set_title(\"Val Accuracy & F1-Macro\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=120)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(\n",
        "    cm: np.ndarray,\n",
        "    class_names: List[str],\n",
        "    out_path: Path,\n",
        "    title: str = \"Confusion Matrix\",\n",
        ") -> None:\n",
        "    try:\n",
        "        import matplotlib\n",
        "        matplotlib.use(\"Agg\")\n",
        "        import matplotlib.pyplot as plt\n",
        "    except ImportError:\n",
        "        print(\"[logger] matplotlib not installed — skipping confusion matrix plot.\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    ax.set(\n",
        "        xticks=range(len(class_names)),\n",
        "        yticks=range(len(class_names)),\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        xlabel=\"Predicted\",\n",
        "        ylabel=\"True\",\n",
        "        title=title,\n",
        "    )\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(\n",
        "                j, i, format(cm[i, j], \"d\"),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "            )\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=120)\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "841ac18d",
      "metadata": {
        "id": "841ac18d"
      },
      "source": [
        "## 9. Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc378e3b",
      "metadata": {
        "id": "bc378e3b"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        cfg: TrainConfig,\n",
        "        class_names: List[str],\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.cfg = cfg\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # Device\n",
        "        self.device = self._resolve_device(cfg.device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Optimiser\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=cfg.lr,\n",
        "            weight_decay=cfg.weight_decay,\n",
        "        )\n",
        "\n",
        "        # LR Scheduler\n",
        "        self.scheduler = self._build_scheduler()\n",
        "\n",
        "        # Logging\n",
        "        run_dir = cfg.run_dir()\n",
        "        self.csv_logger = CSVLogger(run_dir / \"metrics.csv\")\n",
        "        self.best_ckpt_path = run_dir / \"best_checkpoint.pt\"\n",
        "        self._best_f1: float = -1.0\n",
        "\n",
        "    @staticmethod\n",
        "    def _resolve_device(requested: Optional[str]) -> torch.device:\n",
        "        if requested:\n",
        "            return torch.device(requested)\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.device(\"cuda\")\n",
        "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "            return torch.device(\"mps\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "    def _build_scheduler(self):\n",
        "        if self.cfg.lr_scheduler == \"cosine\":\n",
        "            return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "                self.optimizer, T_max=self.cfg.epochs\n",
        "            )\n",
        "        elif self.cfg.lr_scheduler == \"step\":\n",
        "            return torch.optim.lr_scheduler.StepLR(\n",
        "                self.optimizer,\n",
        "                step_size=self.cfg.lr_step_size,\n",
        "                gamma=self.cfg.lr_gamma,\n",
        "            )\n",
        "        return None\n",
        "\n",
        "    def _train_epoch(self, epoch: int) -> float:\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in self.train_loader:\n",
        "            images = images.to(self.device, non_blocking=True)\n",
        "            labels = labels.to(self.device, non_blocking=True)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model(images)\n",
        "            loss = self.criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        return running_loss / len(self.train_loader)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _val_epoch(self) -> Tuple[float, dict, List[int], List[int]]:\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        all_labels: List[int] = []\n",
        "        all_preds: List[int] = []\n",
        "\n",
        "        for images, labels in self.val_loader:\n",
        "            images = images.to(self.device, non_blocking=True)\n",
        "            labels = labels.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = self.model(images)\n",
        "            loss = self.criterion(logits, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "\n",
        "        avg_loss = running_loss / len(self.val_loader)\n",
        "        metrics = compute_metrics(all_labels, all_preds, self.class_names)\n",
        "\n",
        "        return avg_loss, metrics, all_labels, all_preds\n",
        "\n",
        "    def _save_best_checkpoint(self, val_f1: float, epoch: int) -> bool:\n",
        "        if val_f1 > self._best_f1:\n",
        "            self._best_f1 = val_f1\n",
        "            if hasattr(self.model, \"save\"):\n",
        "                self.model.save(self.best_ckpt_path)\n",
        "            else:\n",
        "                torch.save(\n",
        "                    {\"model_state_dict\": self.model.state_dict(), \"epoch\": epoch},\n",
        "                    self.best_ckpt_path,\n",
        "                )\n",
        "            print(f\"  ★ New best F1={val_f1:.4f} — checkpoint saved\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def fit(self) -> None:\n",
        "      print(f\"\\n{'='*60}\")\n",
        "      print(f\"  Model    : {self.cfg.model_name}\")\n",
        "      print(f\"  Device   : {self.device}\")\n",
        "      print(f\"  Epochs   : {self.cfg.epochs}\")\n",
        "      print(f\"  LR       : {self.cfg.lr}  scheduler={self.cfg.lr_scheduler}\")\n",
        "      print(f\"  Run dir  : {self.cfg.run_dir()}\")\n",
        "      print(f\"{'='*60}\\n\")\n",
        "\n",
        "      last_val_labels: List[int] = []\n",
        "      last_val_preds: List[int] = []\n",
        "\n",
        "      # ------------------------------\n",
        "      # Early stopping state\n",
        "      # ------------------------------\n",
        "      early_metric = getattr(self.cfg, \"early_stop_metric\", \"val_loss\")  # \"val_loss\" or \"val_f1_macro\"\n",
        "      patience = getattr(self.cfg, \"early_stop_patience\", 3)\n",
        "      min_delta = getattr(self.cfg, \"early_stop_min_delta\", 1e-4)\n",
        "\n",
        "      if early_metric == \"val_loss\":\n",
        "          best_score = float(\"inf\")\n",
        "      else:\n",
        "          best_score = -float(\"inf\")\n",
        "\n",
        "      best_epoch = -1\n",
        "      patience_counter = 0\n",
        "\n",
        "      for epoch in range(1, self.cfg.epochs + 1):\n",
        "          print(f\"── Epoch {epoch}/{self.cfg.epochs} ──\")\n",
        "\n",
        "          train_loss = self._train_epoch(epoch)\n",
        "          val_loss, val_metrics, val_labels, val_preds = self._val_epoch()\n",
        "          last_val_labels, last_val_preds = val_labels, val_preds\n",
        "\n",
        "          if self.scheduler is not None:\n",
        "              self.scheduler.step()\n",
        "\n",
        "          self.csv_logger.log(\n",
        "              epoch=epoch, split=\"train\",\n",
        "              loss=train_loss, accuracy=0.0, f1_macro=0.0,\n",
        "          )\n",
        "          self.csv_logger.log(\n",
        "              epoch=epoch, split=\"val\",\n",
        "              loss=val_loss, **val_metrics,\n",
        "          )\n",
        "\n",
        "          print(\n",
        "              f\"  train_loss={train_loss:.4f}  \"\n",
        "              f\"val_loss={val_loss:.4f}  \"\n",
        "              f\"val_acc={val_metrics['accuracy']:.4f}  \"\n",
        "              f\"val_f1={val_metrics['f1_macro']:.4f}\"\n",
        "          )\n",
        "\n",
        "          # ------------------------------\n",
        "          # Early stopping check\n",
        "          # ------------------------------\n",
        "          if early_metric == \"val_loss\":\n",
        "              current = float(val_loss)\n",
        "              improved = (best_score - current) > min_delta\n",
        "          else:\n",
        "              current = float(val_metrics[\"f1_macro\"])\n",
        "              improved = (current - best_score) > min_delta\n",
        "\n",
        "          if improved:\n",
        "              best_score = current\n",
        "              best_epoch = epoch\n",
        "              patience_counter = 0\n",
        "\n",
        "              # Save best checkpoint when improved.\n",
        "              # ถ้าคุณยังอยากเก็บ best_f1 แบบเดิม ให้เรียกของเดิมต่อไปได้\n",
        "              self._save_best_checkpoint(val_metrics[\"f1_macro\"], epoch)\n",
        "          else:\n",
        "              patience_counter += 1\n",
        "\n",
        "          if patience_counter >= patience:\n",
        "              print(\n",
        "                  f\"Early stopping at epoch {epoch} \"\n",
        "                  f\"(best_epoch={best_epoch}, best_{early_metric}={best_score:.4f})\"\n",
        "              )\n",
        "              break\n",
        "\n",
        "      # Post-training outputs\n",
        "      run_dir = self.cfg.run_dir()\n",
        "      csv_path = run_dir / \"metrics.csv\"\n",
        "\n",
        "      print(f\"\\n── Generating plots → {run_dir} ──\")\n",
        "      plot_loss_curves(csv_path, run_dir / \"loss_curve.png\")\n",
        "      plot_accuracy_curve(csv_path, run_dir / \"accuracy_curve.png\")\n",
        "\n",
        "      cm = get_confusion_matrix(last_val_labels, last_val_preds, self.class_names)\n",
        "      plot_confusion_matrix(\n",
        "          cm, self.class_names,\n",
        "          run_dir / \"confusion_matrix.png\",\n",
        "          title=f\"Confusion Matrix — {self.cfg.model_name}\",\n",
        "      )\n",
        "\n",
        "      print(f\"\\n── Final Classification Report (val) ──\")\n",
        "      report = get_classification_report(\n",
        "          last_val_labels, last_val_preds, self.class_names\n",
        "      )\n",
        "      print(report)\n",
        "\n",
        "      print(f\"\\n── Confusion Matrix (val) ──\")\n",
        "      _print_confusion_matrix(cm, self.class_names)\n",
        "\n",
        "      print(f\"\\nBest val F1-macro : {self._best_f1:.4f}\")\n",
        "      print(f\"Best checkpoint   : {self.best_ckpt_path}\")\n",
        "      print(f\"Metrics CSV       : {csv_path}\")\n",
        "\n",
        "\n",
        "def _print_confusion_matrix(cm, class_names: List[str]) -> None:\n",
        "    col_w = max(12, max(len(n) for n in class_names) + 2)\n",
        "    header = \" \" * col_w + \"\".join(f\"{n:>{col_w}}\" for n in class_names)\n",
        "    print(header)\n",
        "    for i, name in enumerate(class_names):\n",
        "        row_str = f\"{name:<{col_w}}\" + \"\".join(\n",
        "            f\"{cm[i,j]:>{col_w}}\" for j in range(len(class_names))\n",
        "        )\n",
        "        print(row_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c8280b",
      "metadata": {
        "id": "b9c8280b"
      },
      "source": [
        "## 10. Data Preparation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd7b150",
      "metadata": {
        "id": "abd7b150"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data_cfg: Optional[DataConfig] = None) -> dict:\n",
        "    \"\"\"Run the full data preparation pipeline. Returns the paths dict.\"\"\"\n",
        "    if data_cfg is None:\n",
        "        data_cfg = DataConfig()\n",
        "\n",
        "    # Use HF_TOKEN from environment if not set\n",
        "    if data_cfg.token is None:\n",
        "        data_cfg.token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "    p = data_cfg.paths()\n",
        "    p[\"raw_dir\"].mkdir(parents=True, exist_ok=True)\n",
        "    p[\"proc_dir\"].mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Check if already prepared\n",
        "    if p[\"manifest_clean\"].exists() and p[\"label_map\"].exists():\n",
        "        print(\"✓ Data already prepared. Skipping download & processing.\")\n",
        "        return p\n",
        "\n",
        "    print(\"Downloading raw tar from Hugging Face …\")\n",
        "    raw_tar = download_raw_tar(\n",
        "        repo_id=data_cfg.repo_id,\n",
        "        path_in_repo=data_cfg.raw_tar_in_repo,\n",
        "        repo_type=data_cfg.repo_type,\n",
        "        revision=data_cfg.revision,\n",
        "        token=data_cfg.token,\n",
        "    )\n",
        "\n",
        "    print(\"Extracting …\")\n",
        "    ensure_extracted(raw_tar, p[\"raw_dir\"])\n",
        "\n",
        "    if not p[\"raw_metadata\"].exists():\n",
        "        raise FileNotFoundError(f\"metadata.csv not found at {p['raw_metadata']}\")\n",
        "\n",
        "    print(\"Loading metadata …\")\n",
        "    df = load_metadata(p[\"raw_metadata\"])\n",
        "    df = add_paths(df, p[\"raw_dir\"])\n",
        "\n",
        "    print(\"Cleaning …\")\n",
        "    df = basic_clean(\n",
        "        df,\n",
        "        labels=data_cfg.labels,\n",
        "        dedup_by_barcode=data_cfg.dedup_by_barcode,\n",
        "        cap_per_label=data_cfg.cap_per_label,\n",
        "        seed=data_cfg.seed,\n",
        "    )\n",
        "\n",
        "    print(\"Validating images …\")\n",
        "    df = validate_images(\n",
        "        df,\n",
        "        min_side=data_cfg.min_side,\n",
        "        do_verify=data_cfg.do_verify,\n",
        "        num_workers=data_cfg.num_workers,\n",
        "    )\n",
        "    df = keep_only_ok(df)\n",
        "\n",
        "    print(\"Splitting by barcode …\")\n",
        "    split_cfg = SplitConfig(\n",
        "        seed=data_cfg.seed,\n",
        "        train_frac=data_cfg.train_frac,\n",
        "        val_frac=data_cfg.val_frac,\n",
        "        test_frac=data_cfg.test_frac,\n",
        "    )\n",
        "    df, split_meta = split_by_barcode(df, split_cfg)\n",
        "\n",
        "    # Save outputs\n",
        "    df.to_csv(p[\"manifest_clean\"], index=False)\n",
        "    save_splits_json(split_meta, p[\"splits\"])\n",
        "\n",
        "    label_map = attach_label_map(data_cfg.labels)\n",
        "    p[\"label_map\"].write_text(\n",
        "        json.dumps(label_map, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    stats = compute_stats(df)\n",
        "    save_stats(stats, p[\"stats\"])\n",
        "\n",
        "    print(f\"✓ Data preparation complete!\")\n",
        "    print(f\"  manifest : {p['manifest_clean']}\")\n",
        "    print(f\"  label_map: {p['label_map']}\")\n",
        "    print(f\"  stats    : {p['stats']}\")\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adee701f",
      "metadata": {
        "id": "adee701f"
      },
      "source": [
        "## 11. Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac198193",
      "metadata": {
        "id": "ac198193"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bc9f77f",
      "metadata": {
        "id": "5bc9f77f"
      },
      "source": [
        "## 12. Run Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186331c9",
      "metadata": {
        "id": "186331c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "b88d45c503e84972ad209b158e60a4d8",
            "08d28f3d589d4230be571b2e715a608a",
            "5ef0e4211be54197bca46aae249d8131",
            "3395f1f4375d4092bf0bf2f0d9255c00",
            "7b4d0695b6c4456e8da59cecc5f17a93",
            "c7d24dea9ae84db09efc28dcba1a0ecb",
            "6c63ee8ad4ac4e829e0e2e800be02dd1",
            "bb077cdf999e4eccb234df376d2bc6f4",
            "3efadc6f53b44adc8d5ff1bbb06cc9c7",
            "49d7e00fef62478aae50b78bccda2d1f",
            "18bda811158244b78af3d2bb0df796a9"
          ]
        },
        "outputId": "59fff09f-c364-4c68-ebf0-02c147056f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading raw tar from Hugging Face …\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/raw/data_v2.tar:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b88d45c503e84972ad209b158e60a4d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3279711131.py:12: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tf.extractall(out_dir)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading metadata …\n",
            "Cleaning …\n",
            "Validating images …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validate images: 100%|██████████| 8000/8000 [00:01<00:00, 4019.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting by barcode …\n",
            "✓ Data preparation complete!\n",
            "  manifest : data_local/processed/data_v2/manifest_clean.csv\n",
            "  label_map: data_local/processed/data_v2/label_map.json\n",
            "  stats    : data_local/processed/data_v2/stats.json\n"
          ]
        }
      ],
      "source": [
        "data_cfg = DataConfig()\n",
        "paths = prepare_data(data_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eebf619f",
      "metadata": {
        "id": "eebf619f"
      },
      "source": [
        "## 13. Configure Training\n",
        "\n",
        "Edit these parameters as needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514b93cc",
      "metadata": {
        "id": "514b93cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad107ad5-d3c1-40f0-8e6b-99fa5cb75e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models: ['efficientnet_b0', 'simple_cnn', 'resnet18', 'resnet50', 'mobilenetv2', 'mobilenetv3_large']\n",
            "Selected model  : resnet50\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║                    TRAINING SETTINGS                        ║\n",
        "# ║  Edit these values to control your experiment               ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "\n",
        "MODEL_NAME = \"efficientnet_b0\"          # \"efficientnet_b0\" | \"resnet18\" | \"mobilenetv2\" | \"simple_cnn\"\n",
        "FREEZE_BACKBONE = True           # Stage 1: head-only training\n",
        "DROPOUT = 0.3\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-2\n",
        "LR_SCHEDULER = \"cosine\"         # \"cosine\" | \"step\" | \"none\"\n",
        "IMAGE_SIZE = 224\n",
        "NUM_WORKERS = 2                  # Colab typically has 2 CPUs\n",
        "SEED = 42\n",
        "OUTPUT_DIR = Path(\"outputs\")\n",
        "\n",
        "print(f\"Available models: {available_models()}\")\n",
        "print(f\"Selected model  : {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88e9c31a",
      "metadata": {
        "id": "88e9c31a"
      },
      "source": [
        "## 14. Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6fa736d",
      "metadata": {
        "id": "d6fa736d"
      },
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "\n",
        "# --------- FIX PATHS (paths ของคุณเป็น list) ----------\n",
        "MANIFEST_PATH = \"data_local/processed/data_v2/manifest_clean.csv\"\n",
        "LABEL_MAP_PATH = \"data_local/processed/data_v2/label_map.json\"\n",
        "\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# Stage 1: train head only (freeze backbone)\n",
        "# ======================================================\n",
        "cfg1 = TrainConfig(\n",
        "    model_name=MODEL_NAME,\n",
        "    freeze_backbone=True,\n",
        "    dropout=DROPOUT,\n",
        "    manifest=MANIFEST_PATH,\n",
        "    label_map=LABEL_MAP_PATH,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    lr_scheduler=LR_SCHEDULER,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    seed=SEED,\n",
        "    device=None,\n",
        ")\n",
        "\n",
        "print(\"Loading datasets …\")\n",
        "datasets = build_datasets(\n",
        "    manifest_path=cfg1.manifest,\n",
        "    label_map_path=cfg1.label_map,\n",
        "    train_transform=get_train_transforms(size=cfg1.image_size),\n",
        "    val_transform=get_val_transforms(size=cfg1.image_size),\n",
        ")\n",
        "\n",
        "train_ds = datasets[\"train\"]\n",
        "val_ds = datasets[\"val\"]\n",
        "\n",
        "print(f\"  train : {train_ds}\")\n",
        "print(f\"  val   : {val_ds}\")\n",
        "\n",
        "class_names = train_ds.classes\n",
        "num_classes = train_ds.num_classes\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=cfg1.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg1.num_workers,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=cfg1.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg1.num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "print(f\"Building model: {cfg1.model_name} …\")\n",
        "model = build_model(\n",
        "    name=cfg1.model_name,\n",
        "    num_classes=num_classes,\n",
        "    freeze_backbone=cfg1.freeze_backbone,\n",
        "    dropout=cfg1.dropout,\n",
        ")\n",
        "\n",
        "if hasattr(model, \"param_summary\"):\n",
        "    ps = model.param_summary()\n",
        "    print(f\"[Stage1] params total={ps['total']:,} trainable={ps['trainable']:,} frozen={ps['frozen']:,}\")\n",
        "\n",
        "trainer1 = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    cfg=cfg1,\n",
        "    class_names=class_names,\n",
        ")\n",
        "trainer1.fit()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(SEED)\n",
        "\n",
        "MANIFEST_PATH = \"data_local/processed/data_v2/manifest_clean.csv\"\n",
        "LABEL_MAP_PATH = \"data_local/processed/data_v2/label_map.json\"\n",
        "\n",
        "MODEL_NAME = \"resnet50\"   # หรือ \"mobilenetv3_large\"\n",
        "\n",
        "STAGE1_EPOCHS = 3\n",
        "STAGE2_EPOCHS = 20  # รวมแล้ว 23 epochs (ปรับได้)\n",
        "\n",
        "# ---------- Build datasets once ----------\n",
        "print(\"Loading datasets …\")\n",
        "datasets = build_datasets(\n",
        "    manifest_path=MANIFEST_PATH,\n",
        "    label_map_path=LABEL_MAP_PATH,\n",
        "    train_transform=get_train_transforms(size=IMAGE_SIZE),\n",
        "    val_transform=get_val_transforms(size=IMAGE_SIZE),\n",
        ")\n",
        "\n",
        "train_ds = datasets[\"train\"]\n",
        "val_ds   = datasets[\"val\"]\n",
        "\n",
        "class_names = train_ds.classes\n",
        "num_classes = train_ds.num_classes\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# Stage 1: head-only warmup\n",
        "# ======================================================\n",
        "cfg1 = TrainConfig(\n",
        "    model_name=MODEL_NAME,\n",
        "    freeze_backbone=True,\n",
        "    dropout=DROPOUT,\n",
        "    manifest=MANIFEST_PATH,\n",
        "    label_map=LABEL_MAP_PATH,\n",
        "    epochs=STAGE1_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    lr=1e-3,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    lr_scheduler=LR_SCHEDULER,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    seed=SEED,\n",
        "    device=None,\n",
        "\n",
        "    early_stop_patience=2,\n",
        "    early_stop_min_delta=1e-4,\n",
        "    early_stop_metric=\"val_loss\",\n",
        ")\n",
        "\n",
        "# --- ส่วนที่ต้องแก้ไขใน Cell 14 (หาตำแหน่งหลังจาก build_model) ---\n",
        "\n",
        "# Build model\n",
        "print(f\"Building model: {cfg.model_name} …\")\n",
        "model = build_model(\n",
        "    name=cfg.model_name,\n",
        "    num_classes=num_classes,\n",
        "    freeze_backbone=cfg.freeze_backbone,\n",
        "    dropout=cfg.dropout,\n",
        ")\n",
        "\n",
        "# --- เพิ่มส่วนนี้เข้าไป ---\n",
        "# ตรวจสอบว่าถ้าเป็น EfficientNet ให้ Unfreeze 3 บล็อกสุดท้าย\n",
        "if cfg.model_name == \"efficientnet_b0\":\n",
        "    print(\"Unfreezing last 3 blocks of the backbone for fine-tuning...\")\n",
        "    model.unfreeze_last_n_blocks(n=3)\n",
        "# หากคุณใช้ ResNet หรือรุ่นอื่น ให้ใช้คำสั่ง unfreeze ทั้งหมดแทน (แต่ต้องใช้ LR ต่ำๆ)\n",
        "else:\n",
        "    print(\"Unfreezing the entire backbone...\")\n",
        "    model.unfreeze_backbone()\n",
        "# ------------------------\n",
        "\n",
        "if hasattr(model, \"param_summary\"):\n",
        "    ps = model.param_summary()\n",
        "    print(f\"  params  total={ps['total']:,}  trainable={ps['trainable']:,}  frozen={ps['frozen']:,}\")\n",
        "\n",
        "# หลังจากนี้ก็ใช้โค้ด Trainer เดิมของคุณ...\n",
        "\n",
        "cfg2 = TrainConfig(\n",
        "    model_name=MODEL_NAME,\n",
        "    freeze_backbone=False,\n",
        "    dropout=DROPOUT,\n",
        "    manifest=MANIFEST_PATH,\n",
        "    label_map=LABEL_MAP_PATH,\n",
        "    epochs=STAGE2_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    lr=5e-5,               # <<< สำคัญ: ลด LR ตอน unfreeze (ลอง 3e-5, 5e-5, 1e-4)\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    lr_scheduler=LR_SCHEDULER,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    seed=SEED,\n",
        "    device=str(trainer1.device) if hasattr(trainer1, \"device\") else None,\n",
        "\n",
        "    early_stop_patience=3,\n",
        "    early_stop_min_delta=1e-4,\n",
        "    early_stop_metric=\"val_loss\",\n",
        ")\n",
        "\n",
        "# สร้าง Trainer ใหม่เพื่อ reset optimizer ให้เห็น params ที่ unfreeze แล้ว\n",
        "trainer2 = Trainer(model, train_loader, val_loader, cfg2, class_names)\n",
        "trainer2.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hgQyOR9wbXx",
        "outputId": "e44f6f2a-c3bf-41c0-9f06-e680ec0b1db5"
      },
      "id": "9hgQyOR9wbXx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets …\n",
            "Building model: resnet50 (Stage1) …\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 81.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "  Model    : resnet50\n",
            "  Device   : cuda\n",
            "  Epochs   : 3\n",
            "  LR       : 0.001  scheduler=cosine\n",
            "  Run dir  : outputs/resnet50\n",
            "============================================================\n",
            "\n",
            "── Epoch 1/3 ──\n",
            "  train_loss=1.0993  val_loss=0.9497  val_acc=0.6242  val_f1=0.6227\n",
            "  ★ New best F1=0.6227 — checkpoint saved\n",
            "── Epoch 2/3 ──\n",
            "  train_loss=0.9170  val_loss=0.8941  val_acc=0.6392  val_f1=0.6390\n",
            "  ★ New best F1=0.6390 — checkpoint saved\n",
            "── Epoch 3/3 ──\n",
            "  train_loss=0.8805  val_loss=0.8801  val_acc=0.6533  val_f1=0.6520\n",
            "  ★ New best F1=0.6520 — checkpoint saved\n",
            "\n",
            "── Generating plots → outputs/resnet50 ──\n",
            "\n",
            "── Final Classification Report (val) ──\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   beverages       0.61      0.66      0.63       300\n",
            "      snacks       0.72      0.60      0.65       300\n",
            "    dry_food       0.66      0.59      0.62       300\n",
            "    non_food       0.64      0.77      0.70       300\n",
            "\n",
            "    accuracy                           0.65      1200\n",
            "   macro avg       0.66      0.65      0.65      1200\n",
            "weighted avg       0.66      0.65      0.65      1200\n",
            "\n",
            "\n",
            "── Confusion Matrix (val) ──\n",
            "               beverages      snacks    dry_food    non_food\n",
            "beverages            198          22          28          52\n",
            "snacks                38         179          50          33\n",
            "dry_food              46          33         177          44\n",
            "non_food              42          16          12         230\n",
            "\n",
            "Best val F1-macro : 0.6520\n",
            "Best checkpoint   : outputs/resnet50/best_checkpoint.pt\n",
            "Metrics CSV       : outputs/resnet50/metrics.csv\n",
            "\n",
            "============================================================\n",
            "  Model    : resnet50\n",
            "  Device   : cuda\n",
            "  Epochs   : 20\n",
            "  LR       : 5e-05  scheduler=cosine\n",
            "  Run dir  : outputs/resnet50\n",
            "============================================================\n",
            "\n",
            "── Epoch 1/20 ──\n",
            "  train_loss=0.7794  val_loss=0.7612  val_acc=0.7033  val_f1=0.7029\n",
            "  ★ New best F1=0.7029 — checkpoint saved\n",
            "── Epoch 2/20 ──\n",
            "  train_loss=0.5938  val_loss=0.7200  val_acc=0.7225  val_f1=0.7216\n",
            "  ★ New best F1=0.7216 — checkpoint saved\n",
            "── Epoch 3/20 ──\n",
            "  train_loss=0.4526  val_loss=0.7349  val_acc=0.7275  val_f1=0.7279\n",
            "── Epoch 4/20 ──\n",
            "  train_loss=0.3585  val_loss=0.7222  val_acc=0.7433  val_f1=0.7440\n",
            "── Epoch 5/20 ──\n",
            "  train_loss=0.2653  val_loss=0.7994  val_acc=0.7400  val_f1=0.7404\n",
            "Early stopping at epoch 5 (best_epoch=2, best_val_loss=0.7200)\n",
            "\n",
            "── Generating plots → outputs/resnet50 ──\n",
            "\n",
            "── Final Classification Report (val) ──\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   beverages       0.63      0.81      0.71       300\n",
            "      snacks       0.76      0.76      0.76       300\n",
            "    dry_food       0.81      0.63      0.71       300\n",
            "    non_food       0.80      0.76      0.78       300\n",
            "\n",
            "    accuracy                           0.74      1200\n",
            "   macro avg       0.75      0.74      0.74      1200\n",
            "weighted avg       0.75      0.74      0.74      1200\n",
            "\n",
            "\n",
            "── Confusion Matrix (val) ──\n",
            "               beverages      snacks    dry_food    non_food\n",
            "beverages            243          19          15          23\n",
            "snacks                35         228          23          14\n",
            "dry_food              59          34         188          19\n",
            "non_food              48          18           5         229\n",
            "\n",
            "Best val F1-macro : 0.7216\n",
            "Best checkpoint   : outputs/resnet50/best_checkpoint.pt\n",
            "Metrics CSV       : outputs/resnet50/metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0057c9f3",
      "metadata": {
        "id": "0057c9f3"
      },
      "source": [
        "## 15. Display Training Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "310b4cba",
      "metadata": {
        "id": "310b4cba"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image as IPImage, display\n",
        "\n",
        "run_dir = cfg.run_dir()\n",
        "\n",
        "for plot_name in [\"loss_curve.png\", \"accuracy_curve.png\", \"confusion_matrix.png\"]:\n",
        "    plot_path = run_dir / plot_name\n",
        "    if plot_path.exists():\n",
        "        print(f\"\\n{plot_name}:\")\n",
        "        display(IPImage(filename=str(plot_path)))\n",
        "    else:\n",
        "        print(f\"⚠️  {plot_name} not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d183c9f9",
      "metadata": {
        "id": "d183c9f9"
      },
      "source": [
        "## 16. (Optional) Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad589db6",
      "metadata": {
        "id": "ad589db6"
      },
      "outputs": [],
      "source": [
        "test_ds = datasets[\"test\"]\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Load best checkpoint\n",
        "device = trainer.device\n",
        "best_path = trainer.best_ckpt_path\n",
        "\n",
        "if best_path.exists():\n",
        "    print(f\"Loading best checkpoint: {best_path}\")\n",
        "    ckpt = torch.load(best_path, map_location=str(device))\n",
        "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Test\"):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            logits = model(images)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "\n",
        "    test_metrics = compute_metrics(all_labels, all_preds, class_names)\n",
        "    print(f\"\\n── Test Results ──\")\n",
        "    print(f\"  Accuracy : {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  F1-Macro : {test_metrics['f1_macro']:.4f}\")\n",
        "\n",
        "    print(f\"\\n── Test Classification Report ──\")\n",
        "    print(get_classification_report(all_labels, all_preds, class_names))\n",
        "\n",
        "    cm = get_confusion_matrix(all_labels, all_preds, class_names)\n",
        "    plot_confusion_matrix(\n",
        "        cm, class_names,\n",
        "        run_dir / \"test_confusion_matrix.png\",\n",
        "        title=f\"Test Confusion Matrix — {cfg.model_name}\",\n",
        "    )\n",
        "    display(IPImage(filename=str(run_dir / \"test_confusion_matrix.png\")))\n",
        "else:\n",
        "    print(\"No checkpoint found. Train the model first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d1fea7",
      "metadata": {
        "id": "d2d1fea7"
      },
      "source": [
        "## 17. (Optional) Download Model & Results\n",
        "\n",
        "Uncomment to download outputs to your local machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cd016f4",
      "metadata": {
        "id": "4cd016f4"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "#\n",
        "# # Download best checkpoint\n",
        "# if trainer.best_ckpt_path.exists():\n",
        "#     files.download(str(trainer.best_ckpt_path))\n",
        "#\n",
        "# # Download metrics CSV\n",
        "# csv_path = cfg.run_dir() / \"metrics.csv\"\n",
        "# if csv_path.exists():\n",
        "#     files.download(str(csv_path))\n",
        "#\n",
        "# # Or zip the entire outputs folder\n",
        "# !zip -r outputs.zip outputs/\n",
        "# files.download(\"outputs.zip\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b88d45c503e84972ad209b158e60a4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08d28f3d589d4230be571b2e715a608a",
              "IPY_MODEL_5ef0e4211be54197bca46aae249d8131",
              "IPY_MODEL_3395f1f4375d4092bf0bf2f0d9255c00"
            ],
            "layout": "IPY_MODEL_7b4d0695b6c4456e8da59cecc5f17a93"
          }
        },
        "08d28f3d589d4230be571b2e715a608a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d24dea9ae84db09efc28dcba1a0ecb",
            "placeholder": "​",
            "style": "IPY_MODEL_6c63ee8ad4ac4e829e0e2e800be02dd1",
            "value": "data/raw/data_v2.tar: 100%"
          }
        },
        "5ef0e4211be54197bca46aae249d8131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb077cdf999e4eccb234df376d2bc6f4",
            "max": 242104320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3efadc6f53b44adc8d5ff1bbb06cc9c7",
            "value": 242104320
          }
        },
        "3395f1f4375d4092bf0bf2f0d9255c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49d7e00fef62478aae50b78bccda2d1f",
            "placeholder": "​",
            "style": "IPY_MODEL_18bda811158244b78af3d2bb0df796a9",
            "value": " 242M/242M [00:02&lt;00:00, 137MB/s]"
          }
        },
        "7b4d0695b6c4456e8da59cecc5f17a93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7d24dea9ae84db09efc28dcba1a0ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c63ee8ad4ac4e829e0e2e800be02dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb077cdf999e4eccb234df376d2bc6f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3efadc6f53b44adc8d5ff1bbb06cc9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49d7e00fef62478aae50b78bccda2d1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18bda811158244b78af3d2bb0df796a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}