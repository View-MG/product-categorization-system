{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337729b8",
   "metadata": {},
   "source": [
    "# Smart Product Categorization System\n",
    "\n",
    "This notebook consolidates the entire project for training in Google Colab.\n",
    "\n",
    "**Categories:** `beverages`, `snacks`, `dry_food`, `other`\n",
    "\n",
    "**Architecture:** EfficientNet-B0 / ResNet-18 / MobileNet-V2 / SimpleCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518279b",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de210e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision scikit-learn matplotlib pandas Pillow tqdm huggingface-hub python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30127bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Mount Google Drive if you want persistent storage\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b21dbc",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c729427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import tarfile\n",
    "import argparse\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    Callable, Dict, Iterable, List, Optional, Tuple, Union, Literal,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import (\n",
    "    EfficientNet_B0_Weights,\n",
    "    ResNet18_Weights,\n",
    "    MobileNet_V2_Weights,\n",
    ")\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "except ImportError:\n",
    "    print(\"huggingface_hub not installed — manual data upload required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e372bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Set your Hugging Face token here ─────────────────────────────────────────\n",
    "# Option 1: paste directly\n",
    "HF_TOKEN = \"\"  # <-- paste your token or leave empty\n",
    "\n",
    "# Option 2: use Colab secrets\n",
    "if not HF_TOKEN:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if HF_TOKEN:\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    print(\"HF_TOKEN set ✓\")\n",
    "else:\n",
    "    print(\"⚠️  No HF_TOKEN found. Set it above or upload data manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c231480",
   "metadata": {},
   "source": [
    "## 2. Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    repo_id: str = \"Phathanan/product-categorization-system\"\n",
    "    repo_type: str = \"dataset\"\n",
    "    raw_tar_in_repo: str = \"data/raw/data_v1.tar\"\n",
    "    token: Optional[str] = None\n",
    "    revision: Optional[str] = None\n",
    "\n",
    "    dataset_dir: Path = Path(\"data_local\")\n",
    "    raw_extract_dirname: str = \"raw_extracted\"\n",
    "    processed_dirname: str = \"processed\"\n",
    "    raw_metadata_name: str = \"metadata.csv\"\n",
    "\n",
    "    labels: List[str] = field(\n",
    "        default_factory=lambda: [\"beverages\", \"snacks\", \"dry_food\", \"other\"]\n",
    "    )\n",
    "    dedup_by_barcode: bool = False\n",
    "    cap_per_label: Optional[int] = None\n",
    "\n",
    "    min_side: int = 128\n",
    "    do_verify: bool = False\n",
    "    num_workers: int = 8\n",
    "\n",
    "    seed: int = 42\n",
    "    train_frac: float = 0.70\n",
    "    val_frac: float = 0.15\n",
    "    test_frac: float = 0.15\n",
    "\n",
    "    def paths(self) -> dict:\n",
    "        tar_stem = Path(self.raw_tar_in_repo).name\n",
    "        if tar_stem.endswith(\".tar\"):\n",
    "            tar_stem = tar_stem[:-4]\n",
    "        else:\n",
    "            tar_stem = Path(tar_stem).stem\n",
    "\n",
    "        raw_dir = self.dataset_dir / self.raw_extract_dirname / tar_stem\n",
    "        proc_dir = self.dataset_dir / self.processed_dirname / tar_stem\n",
    "\n",
    "        return {\n",
    "            \"raw_dir\": raw_dir,\n",
    "            \"proc_dir\": proc_dir,\n",
    "            \"raw_metadata\": raw_dir / self.raw_metadata_name,\n",
    "            \"manifest_clean\": proc_dir / \"manifest_clean.csv\",\n",
    "            \"splits\": proc_dir / \"splits.json\",\n",
    "            \"stats\": proc_dir / \"stats.json\",\n",
    "            \"label_map\": proc_dir / \"label_map.json\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d168cc",
   "metadata": {},
   "source": [
    "## 3. Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84473abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model_name: str = \"resnet18\"\n",
    "    freeze_backbone: bool = True\n",
    "    dropout: float = 0.3\n",
    "\n",
    "    manifest: Path = Path(\"data_local/processed/data_v1/manifest_clean.csv\")\n",
    "    label_map: Path = Path(\"data_local/processed/data_v1/label_map.json\")\n",
    "\n",
    "    epochs: int = 20\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 2\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "\n",
    "    lr_scheduler: str = \"cosine\"\n",
    "    lr_step_size: int = 7\n",
    "    lr_gamma: float = 0.1\n",
    "\n",
    "    image_size: int = 224\n",
    "\n",
    "    output_dir: Path = Path(\"outputs\")\n",
    "\n",
    "    seed: int = 42\n",
    "    device: Optional[str] = None\n",
    "\n",
    "    def run_dir(self) -> Path:\n",
    "        d = self.output_dir / self.model_name\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8913864",
   "metadata": {},
   "source": [
    "## 4. Data Utilities — Loader, Prepare, Validate, Split, Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── loader.py ────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _extract_tar(tar_path: Path, out_dir: Path) -> None:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    base = out_dir.resolve()\n",
    "    with tarfile.open(tar_path, \"r:*\") as tf:\n",
    "        members = tf.getmembers()\n",
    "        for m in members:\n",
    "            target = (out_dir / m.name).resolve()\n",
    "            if not str(target).startswith(str(base)):\n",
    "                raise RuntimeError(f\"Unsafe path in tar: {m.name}\")\n",
    "        tf.extractall(out_dir)\n",
    "\n",
    "\n",
    "def download_raw_tar(\n",
    "    repo_id: str,\n",
    "    path_in_repo: str,\n",
    "    repo_type: str = \"dataset\",\n",
    "    revision: Optional[str] = None,\n",
    "    token: Optional[str] = None,\n",
    ") -> Path:\n",
    "    return Path(\n",
    "        hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=path_in_repo,\n",
    "            repo_type=repo_type,\n",
    "            revision=revision,\n",
    "            token=token,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_extracted(raw_tar: Path, extract_dir: Path) -> None:\n",
    "    marker = extract_dir / \".extracted.ok\"\n",
    "    if marker.exists():\n",
    "        return\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    _extract_tar(raw_tar, extract_dir)\n",
    "    marker.write_text(\"ok\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── prepare.py ───────────────────────────────────────────────────────────────\n",
    "\n",
    "def norm_barcode(x: object) -> str:\n",
    "    s = re.sub(r\"\\D\", \"\", str(x or \"\"))\n",
    "    return s.zfill(13) if s else \"\"\n",
    "\n",
    "\n",
    "def load_metadata(meta_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(meta_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_paths(df: pd.DataFrame, raw_dir: Path) -> pd.DataFrame:\n",
    "    raw_dir = Path(raw_dir)\n",
    "    images_dir = raw_dir / \"images\"\n",
    "    if not images_dir.exists():\n",
    "        images_dir = raw_dir\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"barcode\"] = df[\"barcode\"].map(norm_barcode)\n",
    "    df[\"image_id\"] = df[\"image_id\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "\n",
    "    rel = (\n",
    "        df[\"image_id\"]\n",
    "        .str.replace(\"/\", os.sep, regex=False)\n",
    "        .str.lstrip(\"\\\\/\")\n",
    "    )\n",
    "\n",
    "    df[\"abs_path\"] = rel.map(lambda r: str(images_dir / r))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def basic_clean(\n",
    "    df: pd.DataFrame,\n",
    "    labels: Optional[list] = None,\n",
    "    dedup_by_barcode: bool = True,\n",
    "    cap_per_label: Optional[int] = None,\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"label_coarse\" not in df.columns:\n",
    "        raise ValueError(\"metadata must contain label_coarse\")\n",
    "\n",
    "    df[\"label_coarse\"] = df[\"label_coarse\"].astype(str).str.strip()\n",
    "    df = df[df[\"barcode\"].astype(str).str.len() > 0]\n",
    "    df = df[df[\"image_id\"].astype(str).str.len() > 0]\n",
    "    df = df[df[\"abs_path\"].astype(str).str.len() > 0]\n",
    "\n",
    "    if labels:\n",
    "        df = df[df[\"label_coarse\"].isin(labels)]\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"abs_path\"], keep=\"first\")\n",
    "\n",
    "    if dedup_by_barcode:\n",
    "        df = df.sort_values([\"barcode\", \"label_coarse\", \"image_id\"])\n",
    "        df = df.drop_duplicates(subset=[\"barcode\"], keep=\"first\")\n",
    "\n",
    "    if cap_per_label is not None:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        kept = []\n",
    "        for lbl, g in df.groupby(\"label_coarse\"):\n",
    "            if len(g) <= cap_per_label:\n",
    "                kept.append(g)\n",
    "            else:\n",
    "                idx = rng.choice(g.index.to_numpy(), size=cap_per_label, replace=False)\n",
    "                kept.append(df.loc[idx])\n",
    "        df = pd.concat(kept, ignore_index=True)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def attach_label_map(labels: list) -> dict:\n",
    "    return {lbl: i for i, lbl in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── validate.py ──────────────────────────────────────────────────────────────\n",
    "\n",
    "def _check_one(path: str, min_side: int, do_verify: bool) -> Tuple[int, int, int, int]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return 0, 0, 0, 0\n",
    "    try:\n",
    "        if do_verify:\n",
    "            with Image.open(p) as im:\n",
    "                im.verify()\n",
    "        with Image.open(p) as im:\n",
    "            w, h = im.size\n",
    "        ok = int(min(w, h) >= min_side)\n",
    "        size = int(p.stat().st_size)\n",
    "        return ok, w, h, size\n",
    "    except Exception:\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "\n",
    "def validate_images(\n",
    "    df: pd.DataFrame,\n",
    "    min_side: int = 128,\n",
    "    do_verify: bool = False,\n",
    "    num_workers: int = 8,\n",
    ") -> pd.DataFrame:\n",
    "    if \"abs_path\" not in df.columns:\n",
    "        raise ValueError(\"df must contain abs_path\")\n",
    "\n",
    "    paths = df[\"abs_path\"].astype(str).tolist()\n",
    "    results = [None] * len(paths)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max(1, int(num_workers))) as ex:\n",
    "        futs = {\n",
    "            ex.submit(_check_one, paths[i], min_side, do_verify): i\n",
    "            for i in range(len(paths))\n",
    "        }\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"validate images\"):\n",
    "            i = futs[fut]\n",
    "            results[i] = fut.result()\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"img_ok\"] = [r[0] for r in results]\n",
    "    out[\"w\"] = [r[1] for r in results]\n",
    "    out[\"h\"] = [r[2] for r in results]\n",
    "    out[\"file_size\"] = [r[3] for r in results]\n",
    "    return out\n",
    "\n",
    "\n",
    "def keep_only_ok(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"img_ok\" not in df.columns:\n",
    "        raise ValueError(\"df must contain img_ok\")\n",
    "    return df[df[\"img_ok\"] == 1].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── split.py ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "@dataclass\n",
    "class SplitConfig:\n",
    "    seed: int = 42\n",
    "    train_frac: float = 0.8\n",
    "    val_frac: float = 0.1\n",
    "    test_frac: float = 0.1\n",
    "\n",
    "\n",
    "def _alloc_counts(n: int, train_f: float, val_f: float, test_f: float) -> Tuple[int, int, int]:\n",
    "    if n <= 0:\n",
    "        return 0, 0, 0\n",
    "    if n == 1:\n",
    "        return 1, 0, 0\n",
    "    if n == 2:\n",
    "        return 1, 1, 0\n",
    "    val = max(1, int(round(n * val_f)))\n",
    "    test = max(1, int(round(n * test_f)))\n",
    "    if val + test >= n:\n",
    "        val = 1\n",
    "        test = 1\n",
    "    train = n - val - test\n",
    "    if train <= 0:\n",
    "        train = max(1, n - 2)\n",
    "        val = 1 if n - train >= 1 else 0\n",
    "        test = n - train - val\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def split_by_barcode(df: pd.DataFrame, cfg: SplitConfig) -> Tuple[pd.DataFrame, Dict]:\n",
    "    if \"barcode\" not in df.columns or \"label_coarse\" not in df.columns:\n",
    "        raise ValueError(\"df must contain barcode and label_coarse\")\n",
    "\n",
    "    rng = np.random.default_rng(cfg.seed)\n",
    "    pairs = df[[\"barcode\", \"label_coarse\"]].drop_duplicates()\n",
    "    barcode_to_label = dict(zip(pairs[\"barcode\"], pairs[\"label_coarse\"]))\n",
    "\n",
    "    split_map: Dict[str, str] = {}\n",
    "\n",
    "    for lbl, g in pairs.groupby(\"label_coarse\"):\n",
    "        barcodes = g[\"barcode\"].tolist()\n",
    "        rng.shuffle(barcodes)\n",
    "\n",
    "        n = len(barcodes)\n",
    "        n_train, n_val, n_test = _alloc_counts(n, cfg.train_frac, cfg.val_frac, cfg.test_frac)\n",
    "\n",
    "        train_ids = barcodes[:n_train]\n",
    "        val_ids = barcodes[n_train: n_train + n_val]\n",
    "        test_ids = barcodes[n_train + n_val: n_train + n_val + n_test]\n",
    "\n",
    "        for b in train_ids:\n",
    "            split_map[b] = \"train\"\n",
    "        for b in val_ids:\n",
    "            split_map[b] = \"val\"\n",
    "        for b in test_ids:\n",
    "            split_map[b] = \"test\"\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"split\"] = out[\"barcode\"].map(split_map).fillna(\"train\")\n",
    "\n",
    "    splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    for b, s in split_map.items():\n",
    "        splits[s].append(b)\n",
    "\n",
    "    meta = {\n",
    "        \"seed\": cfg.seed,\n",
    "        \"fractions\": {\n",
    "            \"train\": cfg.train_frac,\n",
    "            \"val\": cfg.val_frac,\n",
    "            \"test\": cfg.test_frac,\n",
    "        },\n",
    "        \"counts\": {k: len(v) for k, v in splits.items()},\n",
    "        \"splits\": splits,\n",
    "        \"barcode_label\": barcode_to_label,\n",
    "    }\n",
    "    return out, meta\n",
    "\n",
    "\n",
    "def save_splits_json(meta: Dict, out_path: Path) -> None:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_path.write_text(\n",
    "        json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49774e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── stats.py ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "def compute_stats(df: pd.DataFrame) -> Dict:\n",
    "    total = int(len(df))\n",
    "    by_label = (\n",
    "        df[\"label_coarse\"].value_counts().to_dict()\n",
    "        if \"label_coarse\" in df.columns\n",
    "        else {}\n",
    "    )\n",
    "    by_split = (\n",
    "        df[\"split\"].value_counts().to_dict() if \"split\" in df.columns else {}\n",
    "    )\n",
    "\n",
    "    by_label_split = {}\n",
    "    if \"label_coarse\" in df.columns and \"split\" in df.columns:\n",
    "        tmp = df.groupby([\"label_coarse\", \"split\"]).size().reset_index(name=\"n\")\n",
    "        for _, r in tmp.iterrows():\n",
    "            lbl = r[\"label_coarse\"]\n",
    "            if lbl not in by_label_split:\n",
    "                by_label_split[lbl] = {}\n",
    "            by_label_split[lbl][r[\"split\"]] = int(r[\"n\"])\n",
    "\n",
    "    img_ok_rate = None\n",
    "    if \"img_ok\" in df.columns:\n",
    "        img_ok_rate = float(df[\"img_ok\"].mean()) if len(df) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"by_label\": {k: int(v) for k, v in by_label.items()},\n",
    "        \"by_split\": {k: int(v) for k, v in by_split.items()},\n",
    "        \"by_label_split\": by_label_split,\n",
    "        \"img_ok_rate\": img_ok_rate,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_stats(stats: Dict, out_path: Path) -> None:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_path.write_text(\n",
    "        json.dumps(stats, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82881c5",
   "metadata": {},
   "source": [
    "## 5. Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def get_train_transforms(size: int = 224) -> transforms.Compose:\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size + 32),\n",
    "            transforms.RandomResizedCrop(\n",
    "                size,\n",
    "                scale=(0.7, 1.0),\n",
    "                ratio=(0.75, 1.33),\n",
    "                interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "            ),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_val_transforms(size: int = 224) -> transforms.Compose:\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abab676",
   "metadata": {},
   "source": [
    "## 6. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4dea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for product package image classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        manifest: Union[pd.DataFrame, Path, str],\n",
    "        label_map: Union[Dict[str, int], Path, str],\n",
    "        split: Optional[str] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        if isinstance(manifest, (str, Path)):\n",
    "            manifest = pd.read_csv(manifest)\n",
    "\n",
    "        if \"abs_path\" not in manifest.columns:\n",
    "            raise ValueError(\"manifest must contain column 'abs_path'\")\n",
    "        if \"label_coarse\" not in manifest.columns:\n",
    "            raise ValueError(\"manifest must contain column 'label_coarse'\")\n",
    "\n",
    "        if split is not None:\n",
    "            if \"split\" not in manifest.columns:\n",
    "                raise ValueError(\n",
    "                    f\"split='{split}' requested but manifest has no 'split' column.\"\n",
    "                )\n",
    "            manifest = manifest[manifest[\"split\"] == split].copy()\n",
    "            if len(manifest) == 0:\n",
    "                raise ValueError(f\"No rows found for split='{split}'.\")\n",
    "\n",
    "        self._df = manifest.reset_index(drop=True)\n",
    "\n",
    "        if isinstance(label_map, (str, Path)):\n",
    "            label_map = json.loads(Path(label_map).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        self._label_map: Dict[str, int] = label_map\n",
    "\n",
    "        unknown = set(self._df[\"label_coarse\"].unique()) - set(\n",
    "            self._label_map.keys()\n",
    "        )\n",
    "        if unknown:\n",
    "            raise ValueError(\n",
    "                f\"Labels found in manifest but missing from label_map: {unknown}\"\n",
    "            )\n",
    "\n",
    "        self.transform = transform\n",
    "        self.classes: list = sorted(self._label_map, key=self._label_map.get)\n",
    "        self.num_classes: int = len(self._label_map)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        row = self._df.iloc[idx]\n",
    "        img = Image.open(row[\"abs_path\"]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(\n",
    "            self._label_map[row[\"label_coarse\"]], dtype=torch.long\n",
    "        )\n",
    "        return img, label\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"ProductDataset(split={self._df['split'].iloc[0] if 'split' in self._df.columns else '?'}, \"\n",
    "            f\"n={len(self)}, classes={self.num_classes})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def build_datasets(\n",
    "    manifest_path: Union[Path, str],\n",
    "    label_map_path: Union[Path, str],\n",
    "    train_transform: Optional[Callable] = None,\n",
    "    val_transform: Optional[Callable] = None,\n",
    ") -> Dict[str, ProductDataset]:\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "    label_map: Dict[str, int] = json.loads(\n",
    "        Path(label_map_path).read_text(encoding=\"utf-8\")\n",
    "    )\n",
    "\n",
    "    ds: Dict[str, ProductDataset] = {}\n",
    "    for split_name in (\"train\", \"val\", \"test\"):\n",
    "        transform = train_transform if split_name == \"train\" else val_transform\n",
    "        ds[split_name] = ProductDataset(\n",
    "            manifest=manifest,\n",
    "            label_map=label_map,\n",
    "            split=split_name,\n",
    "            transform=transform,\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a13da",
   "metadata": {},
   "source": [
    "## 7. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── ProductClassifier (EfficientNet-B0) ──────────────────────────────────────\n",
    "\n",
    "class ProductClassifier(nn.Module):\n",
    "    BACKBONE_OUT_FEATURES: int = 1280\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 4,\n",
    "        freeze_backbone: bool = True,\n",
    "        dropout: float = 0.3,\n",
    "        pretrained: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        backbone = models.efficientnet_b0(weights=weights)\n",
    "\n",
    "        self.backbone: nn.Module = backbone.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(self.BACKBONE_OUT_FEATURES, num_classes),\n",
    "        )\n",
    "\n",
    "        nn.init.xavier_uniform_(self.head[2].weight)\n",
    "        nn.init.zeros_(self.head[2].bias)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self._backbone_frozen = False\n",
    "\n",
    "        if freeze_backbone:\n",
    "            self.freeze_backbone()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def freeze_backbone(self) -> None:\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._backbone_frozen = True\n",
    "\n",
    "    def unfreeze_backbone(self) -> None:\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        self._backbone_frozen = False\n",
    "\n",
    "    def unfreeze_last_n_blocks(self, n: int = 3) -> None:\n",
    "        blocks = list(self.backbone.children())\n",
    "        for block in blocks[-n:]:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def trainable_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def total_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def param_summary(self) -> Dict[str, int]:\n",
    "        total = self.total_params()\n",
    "        trainable = self.trainable_params()\n",
    "        return {\"total\": total, \"trainable\": trainable, \"frozen\": total - trainable}\n",
    "\n",
    "    def save(self, path: Path) -> None:\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.state_dict(),\n",
    "                \"num_classes\": self.num_classes,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Path, num_classes: Optional[int] = None, map_location: str = \"cpu\") -> \"ProductClassifier\":\n",
    "        ckpt = torch.load(path, map_location=map_location)\n",
    "        nc = num_classes or ckpt[\"num_classes\"]\n",
    "        model = cls(num_classes=nc, freeze_backbone=False, pretrained=False)\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        return model\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"ProductClassifier(\"\n",
    "            f\"backbone=EfficientNet-B0, \"\n",
    "            f\"num_classes={self.num_classes}, \"\n",
    "            f\"frozen={self._backbone_frozen}, \"\n",
    "            f\"trainable_params={self.trainable_params():,})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c63f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SimpleCNN ────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 4, dropout: float = 0.3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def freeze_backbone(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def unfreeze_backbone(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def trainable_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def total_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def param_summary(self) -> dict:\n",
    "        total = self.total_params()\n",
    "        trainable = self.trainable_params()\n",
    "        return {\"total\": total, \"trainable\": trainable, \"frozen\": total - trainable}\n",
    "\n",
    "    def save(self, path) -> None:\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\"model_state_dict\": self.state_dict(), \"num_classes\": self.num_classes},\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"SimpleCNN(num_classes={self.num_classes}, trainable_params={self.trainable_params():,})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── TransferModel wrapper (for ResNet-18 / MobileNet-V2) ────────────────────\n",
    "\n",
    "class _TransferModel(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, num_classes: int, freeze_backbone: bool) -> None:\n",
    "        super().__init__()\n",
    "        self._backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "        self._backbone_frozen = False\n",
    "        if freeze_backbone:\n",
    "            self.freeze_backbone()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._backbone(x)\n",
    "\n",
    "    def freeze_backbone(self) -> None:\n",
    "        for name, param in self._backbone.named_parameters():\n",
    "            if not name.startswith(\"fc.\") and not name.startswith(\"classifier.\"):\n",
    "                param.requires_grad = False\n",
    "        self._backbone_frozen = True\n",
    "\n",
    "    def unfreeze_backbone(self) -> None:\n",
    "        for param in self._backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        self._backbone_frozen = False\n",
    "\n",
    "    def trainable_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def total_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def param_summary(self) -> dict:\n",
    "        total = self.total_params()\n",
    "        trainable = self.trainable_params()\n",
    "        return {\"total\": total, \"trainable\": trainable, \"frozen\": total - trainable}\n",
    "\n",
    "    def save(self, path) -> None:\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\"model_state_dict\": self.state_dict(), \"num_classes\": self.num_classes},\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        name = type(self._backbone).__name__\n",
    "        return (\n",
    "            f\"{name}Wrapper(\"\n",
    "            f\"num_classes={self.num_classes}, \"\n",
    "            f\"frozen={self._backbone_frozen}, \"\n",
    "            f\"trainable_params={self.trainable_params():,})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7309f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Model factory ────────────────────────────────────────────────────────────\n",
    "\n",
    "def _build_resnet18(num_classes: int, freeze_backbone: bool, dropout: float) -> _TransferModel:\n",
    "    backbone = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    in_features = backbone.fc.in_features\n",
    "    backbone.fc = nn.Sequential(\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(in_features, num_classes),\n",
    "    )\n",
    "    nn.init.xavier_uniform_(backbone.fc[1].weight)\n",
    "    nn.init.zeros_(backbone.fc[1].bias)\n",
    "    return _TransferModel(backbone, num_classes, freeze_backbone)\n",
    "\n",
    "\n",
    "def _build_mobilenetv2(num_classes: int, freeze_backbone: bool, dropout: float) -> _TransferModel:\n",
    "    backbone = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    in_features = backbone.classifier[1].in_features\n",
    "    backbone.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(in_features, num_classes),\n",
    "    )\n",
    "    nn.init.xavier_uniform_(backbone.classifier[1].weight)\n",
    "    nn.init.zeros_(backbone.classifier[1].bias)\n",
    "    return _TransferModel(backbone, num_classes, freeze_backbone)\n",
    "\n",
    "\n",
    "_REGISTRY = {\n",
    "    \"efficientnet_b0\": lambda nc, fb, do: ProductClassifier(num_classes=nc, freeze_backbone=fb, dropout=do),\n",
    "    \"simple_cnn\": lambda nc, fb, do: SimpleCNN(num_classes=nc, dropout=do),\n",
    "    \"resnet18\": _build_resnet18,\n",
    "    \"mobilenetv2\": _build_mobilenetv2,\n",
    "}\n",
    "\n",
    "ModelName = Literal[\"efficientnet_b0\", \"simple_cnn\", \"resnet18\", \"mobilenetv2\"]\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    name: str,\n",
    "    num_classes: int = 4,\n",
    "    freeze_backbone: bool = True,\n",
    "    dropout: float = 0.3,\n",
    ") -> nn.Module:\n",
    "    name = name.lower().strip()\n",
    "    if name not in _REGISTRY:\n",
    "        raise ValueError(\n",
    "            f\"Unknown model '{name}'. Choose from: {list(_REGISTRY.keys())}\"\n",
    "        )\n",
    "    return _REGISTRY[name](num_classes, freeze_backbone, dropout)\n",
    "\n",
    "\n",
    "def available_models() -> list:\n",
    "    return list(_REGISTRY.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9868372d",
   "metadata": {},
   "source": [
    "## 8. Metrics & Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── metrics.py ───────────────────────────────────────────────────────────────\n",
    "\n",
    "def compute_metrics(\n",
    "    all_labels: List[int],\n",
    "    all_preds: List[int],\n",
    "    class_names: Optional[List[str]] = None,\n",
    ") -> Dict[str, float]:\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "\n",
    "    result: Dict[str, float] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "    }\n",
    "\n",
    "    if class_names:\n",
    "        per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        for name, score in zip(class_names, per_class):\n",
    "            result[f\"f1_{name}\"] = float(score)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_classification_report(\n",
    "    all_labels: List[int],\n",
    "    all_preds: List[int],\n",
    "    class_names: Optional[List[str]] = None,\n",
    ") -> str:\n",
    "    return classification_report(\n",
    "        all_labels, all_preds, target_names=class_names, zero_division=0\n",
    "    )\n",
    "\n",
    "\n",
    "def get_confusion_matrix(\n",
    "    all_labels: List[int],\n",
    "    all_preds: List[int],\n",
    ") -> np.ndarray:\n",
    "    return confusion_matrix(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7279d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── logger.py ────────────────────────────────────────────────────────────────\n",
    "\n",
    "class CSVLogger:\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        self.path = Path(path)\n",
    "        self._header_written = self.path.exists()\n",
    "\n",
    "    def log(self, epoch: int, split: str, **metrics: float) -> None:\n",
    "        row = {\"epoch\": epoch, \"split\": split, **metrics}\n",
    "        write_header = not self._header_written\n",
    "        with self.path.open(\"a\", newline=\"\") as fh:\n",
    "            writer = csv.DictWriter(fh, fieldnames=list(row.keys()))\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "                self._header_written = True\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def _read_csv(path: Path) -> List[Dict]:\n",
    "    with path.open() as fh:\n",
    "        return list(csv.DictReader(fh))\n",
    "\n",
    "\n",
    "def plot_loss_curves(csv_path: Path, out_path: Path) -> None:\n",
    "    try:\n",
    "        import matplotlib\n",
    "        matplotlib.use(\"Agg\")\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print(\"[logger] matplotlib not installed — skipping loss curve plot.\")\n",
    "        return\n",
    "\n",
    "    rows = _read_csv(csv_path)\n",
    "    train_rows = [r for r in rows if r[\"split\"] == \"train\"]\n",
    "    val_rows = [r for r in rows if r[\"split\"] == \"val\"]\n",
    "\n",
    "    if not train_rows or not val_rows:\n",
    "        return\n",
    "\n",
    "    epochs = [int(r[\"epoch\"]) for r in train_rows]\n",
    "    train_loss = [float(r[\"loss\"]) for r in train_rows]\n",
    "    val_loss = [float(r[\"loss\"]) for r in val_rows]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(epochs, train_loss, label=\"train_loss\", marker=\"o\", markersize=3)\n",
    "    ax.plot(epochs, val_loss, label=\"val_loss\", marker=\"s\", markersize=3)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Train vs Val Loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=120)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_accuracy_curve(csv_path: Path, out_path: Path) -> None:\n",
    "    try:\n",
    "        import matplotlib\n",
    "        matplotlib.use(\"Agg\")\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print(\"[logger] matplotlib not installed — skipping accuracy curve plot.\")\n",
    "        return\n",
    "\n",
    "    rows = _read_csv(csv_path)\n",
    "    val_rows = [r for r in rows if r[\"split\"] == \"val\"]\n",
    "\n",
    "    if not val_rows:\n",
    "        return\n",
    "\n",
    "    epochs = [int(r[\"epoch\"]) for r in val_rows]\n",
    "    accuracy = [float(r[\"accuracy\"]) for r in val_rows]\n",
    "    f1 = [float(r[\"f1_macro\"]) for r in val_rows]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(epochs, accuracy, label=\"val_accuracy\", marker=\"o\", markersize=3)\n",
    "    ax.plot(epochs, f1, label=\"val_f1_macro\", marker=\"s\", markersize=3)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_title(\"Val Accuracy & F1-Macro\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=120)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    cm: np.ndarray,\n",
    "    class_names: List[str],\n",
    "    out_path: Path,\n",
    "    title: str = \"Confusion Matrix\",\n",
    ") -> None:\n",
    "    try:\n",
    "        import matplotlib\n",
    "        matplotlib.use(\"Agg\")\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print(\"[logger] matplotlib not installed — skipping confusion matrix plot.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    ax.set(\n",
    "        xticks=range(len(class_names)),\n",
    "        yticks=range(len(class_names)),\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        xlabel=\"Predicted\",\n",
    "        ylabel=\"True\",\n",
    "        title=title,\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, format(cm[i, j], \"d\"),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=120)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ac18d",
   "metadata": {},
   "source": [
    "## 9. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc378e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        cfg: TrainConfig,\n",
    "        class_names: List[str],\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.cfg = cfg\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # Device\n",
    "        self.device = self._resolve_device(cfg.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Optimiser\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "        )\n",
    "\n",
    "        # LR Scheduler\n",
    "        self.scheduler = self._build_scheduler()\n",
    "\n",
    "        # Logging\n",
    "        run_dir = cfg.run_dir()\n",
    "        self.csv_logger = CSVLogger(run_dir / \"metrics.csv\")\n",
    "        self.best_ckpt_path = run_dir / \"best_checkpoint.pt\"\n",
    "        self._best_f1: float = -1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _resolve_device(requested: Optional[str]) -> torch.device:\n",
    "        if requested:\n",
    "            return torch.device(requested)\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    def _build_scheduler(self):\n",
    "        if self.cfg.lr_scheduler == \"cosine\":\n",
    "            return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer, T_max=self.cfg.epochs\n",
    "            )\n",
    "        elif self.cfg.lr_scheduler == \"step\":\n",
    "            return torch.optim.lr_scheduler.StepLR(\n",
    "                self.optimizer,\n",
    "                step_size=self.cfg.lr_step_size,\n",
    "                gamma=self.cfg.lr_gamma,\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    def _train_epoch(self, epoch: int) -> float:\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in self.train_loader:\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(images)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(self.train_loader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _val_epoch(self) -> Tuple[float, dict, List[int], List[int]]:\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_labels: List[int] = []\n",
    "        all_preds: List[int] = []\n",
    "\n",
    "        for images, labels in self.val_loader:\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "\n",
    "            logits = self.model(images)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        metrics = compute_metrics(all_labels, all_preds, self.class_names)\n",
    "\n",
    "        return avg_loss, metrics, all_labels, all_preds\n",
    "\n",
    "    def _save_best_checkpoint(self, val_f1: float, epoch: int) -> bool:\n",
    "        if val_f1 > self._best_f1:\n",
    "            self._best_f1 = val_f1\n",
    "            if hasattr(self.model, \"save\"):\n",
    "                self.model.save(self.best_ckpt_path)\n",
    "            else:\n",
    "                torch.save(\n",
    "                    {\"model_state_dict\": self.model.state_dict(), \"epoch\": epoch},\n",
    "                    self.best_ckpt_path,\n",
    "                )\n",
    "            print(f\"  ★ New best F1={val_f1:.4f} — checkpoint saved\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  Model    : {self.cfg.model_name}\")\n",
    "        print(f\"  Device   : {self.device}\")\n",
    "        print(f\"  Epochs   : {self.cfg.epochs}\")\n",
    "        print(f\"  LR       : {self.cfg.lr}  scheduler={self.cfg.lr_scheduler}\")\n",
    "        print(f\"  Run dir  : {self.cfg.run_dir()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        last_val_labels: List[int] = []\n",
    "        last_val_preds: List[int] = []\n",
    "\n",
    "        for epoch in range(1, self.cfg.epochs + 1):\n",
    "            print(f\"── Epoch {epoch}/{self.cfg.epochs} ──\")\n",
    "\n",
    "            train_loss = self._train_epoch(epoch)\n",
    "            val_loss, val_metrics, val_labels, val_preds = self._val_epoch()\n",
    "            last_val_labels, last_val_preds = val_labels, val_preds\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            self.csv_logger.log(\n",
    "                epoch=epoch, split=\"train\",\n",
    "                loss=train_loss, accuracy=0.0, f1_macro=0.0,\n",
    "            )\n",
    "            self.csv_logger.log(\n",
    "                epoch=epoch, split=\"val\",\n",
    "                loss=val_loss, **val_metrics,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"  train_loss={train_loss:.4f}  \"\n",
    "                f\"val_loss={val_loss:.4f}  \"\n",
    "                f\"val_acc={val_metrics['accuracy']:.4f}  \"\n",
    "                f\"val_f1={val_metrics['f1_macro']:.4f}\"\n",
    "            )\n",
    "\n",
    "            self._save_best_checkpoint(val_metrics[\"f1_macro\"], epoch)\n",
    "\n",
    "        # Post-training outputs\n",
    "        run_dir = self.cfg.run_dir()\n",
    "        csv_path = run_dir / \"metrics.csv\"\n",
    "\n",
    "        print(f\"\\n── Generating plots → {run_dir} ──\")\n",
    "        plot_loss_curves(csv_path, run_dir / \"loss_curve.png\")\n",
    "        plot_accuracy_curve(csv_path, run_dir / \"accuracy_curve.png\")\n",
    "\n",
    "        cm = get_confusion_matrix(last_val_labels, last_val_preds)\n",
    "        plot_confusion_matrix(\n",
    "            cm, self.class_names,\n",
    "            run_dir / \"confusion_matrix.png\",\n",
    "            title=f\"Confusion Matrix — {self.cfg.model_name}\",\n",
    "        )\n",
    "\n",
    "        print(f\"\\n── Final Classification Report (val) ──\")\n",
    "        report = get_classification_report(\n",
    "            last_val_labels, last_val_preds, self.class_names\n",
    "        )\n",
    "        print(report)\n",
    "\n",
    "        print(f\"\\n── Confusion Matrix (val) ──\")\n",
    "        _print_confusion_matrix(cm, self.class_names)\n",
    "\n",
    "        print(f\"\\nBest val F1-macro : {self._best_f1:.4f}\")\n",
    "        print(f\"Best checkpoint   : {self.best_ckpt_path}\")\n",
    "        print(f\"Metrics CSV       : {csv_path}\")\n",
    "\n",
    "\n",
    "def _print_confusion_matrix(cm, class_names: List[str]) -> None:\n",
    "    col_w = max(12, max(len(n) for n in class_names) + 2)\n",
    "    header = \" \" * col_w + \"\".join(f\"{n:>{col_w}}\" for n in class_names)\n",
    "    print(header)\n",
    "    for i, name in enumerate(class_names):\n",
    "        row_str = f\"{name:<{col_w}}\" + \"\".join(\n",
    "            f\"{cm[i,j]:>{col_w}}\" for j in range(len(class_names))\n",
    "        )\n",
    "        print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8280b",
   "metadata": {},
   "source": [
    "## 10. Data Preparation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_cfg: Optional[DataConfig] = None) -> dict:\n",
    "    \"\"\"Run the full data preparation pipeline. Returns the paths dict.\"\"\"\n",
    "    if data_cfg is None:\n",
    "        data_cfg = DataConfig()\n",
    "\n",
    "    # Use HF_TOKEN from environment if not set\n",
    "    if data_cfg.token is None:\n",
    "        data_cfg.token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "    p = data_cfg.paths()\n",
    "    p[\"raw_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "    p[\"proc_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check if already prepared\n",
    "    if p[\"manifest_clean\"].exists() and p[\"label_map\"].exists():\n",
    "        print(\"✓ Data already prepared. Skipping download & processing.\")\n",
    "        return p\n",
    "\n",
    "    print(\"Downloading raw tar from Hugging Face …\")\n",
    "    raw_tar = download_raw_tar(\n",
    "        repo_id=data_cfg.repo_id,\n",
    "        path_in_repo=data_cfg.raw_tar_in_repo,\n",
    "        repo_type=data_cfg.repo_type,\n",
    "        revision=data_cfg.revision,\n",
    "        token=data_cfg.token,\n",
    "    )\n",
    "\n",
    "    print(\"Extracting …\")\n",
    "    ensure_extracted(raw_tar, p[\"raw_dir\"])\n",
    "\n",
    "    if not p[\"raw_metadata\"].exists():\n",
    "        raise FileNotFoundError(f\"metadata.csv not found at {p['raw_metadata']}\")\n",
    "\n",
    "    print(\"Loading metadata …\")\n",
    "    df = load_metadata(p[\"raw_metadata\"])\n",
    "    df = add_paths(df, p[\"raw_dir\"])\n",
    "\n",
    "    print(\"Cleaning …\")\n",
    "    df = basic_clean(\n",
    "        df,\n",
    "        labels=data_cfg.labels,\n",
    "        dedup_by_barcode=data_cfg.dedup_by_barcode,\n",
    "        cap_per_label=data_cfg.cap_per_label,\n",
    "        seed=data_cfg.seed,\n",
    "    )\n",
    "\n",
    "    print(\"Validating images …\")\n",
    "    df = validate_images(\n",
    "        df,\n",
    "        min_side=data_cfg.min_side,\n",
    "        do_verify=data_cfg.do_verify,\n",
    "        num_workers=data_cfg.num_workers,\n",
    "    )\n",
    "    df = keep_only_ok(df)\n",
    "\n",
    "    print(\"Splitting by barcode …\")\n",
    "    split_cfg = SplitConfig(\n",
    "        seed=data_cfg.seed,\n",
    "        train_frac=data_cfg.train_frac,\n",
    "        val_frac=data_cfg.val_frac,\n",
    "        test_frac=data_cfg.test_frac,\n",
    "    )\n",
    "    df, split_meta = split_by_barcode(df, split_cfg)\n",
    "\n",
    "    # Save outputs\n",
    "    df.to_csv(p[\"manifest_clean\"], index=False)\n",
    "    save_splits_json(split_meta, p[\"splits\"])\n",
    "\n",
    "    label_map = attach_label_map(data_cfg.labels)\n",
    "    p[\"label_map\"].write_text(\n",
    "        json.dumps(label_map, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    stats = compute_stats(df)\n",
    "    save_stats(stats, p[\"stats\"])\n",
    "\n",
    "    print(f\"✓ Data preparation complete!\")\n",
    "    print(f\"  manifest : {p['manifest_clean']}\")\n",
    "    print(f\"  label_map: {p['label_map']}\")\n",
    "    print(f\"  stats    : {p['stats']}\")\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee701f",
   "metadata": {},
   "source": [
    "## 11. Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac198193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9f77f",
   "metadata": {},
   "source": [
    "## 12. Run Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186331c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = DataConfig()\n",
    "paths = prepare_data(data_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf619f",
   "metadata": {},
   "source": [
    "## 13. Configure Training\n",
    "\n",
    "Edit these parameters as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║                    TRAINING SETTINGS                        ║\n",
    "# ║  Edit these values to control your experiment               ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "MODEL_NAME = \"resnet18\"          # \"efficientnet_b0\" | \"resnet18\" | \"mobilenetv2\" | \"simple_cnn\"\n",
    "FREEZE_BACKBONE = True           # Stage 1: head-only training\n",
    "DROPOUT = 0.3\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LR_SCHEDULER = \"cosine\"         # \"cosine\" | \"step\" | \"none\"\n",
    "IMAGE_SIZE = 224\n",
    "NUM_WORKERS = 2                  # Colab typically has 2 CPUs\n",
    "SEED = 42\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "\n",
    "print(f\"Available models: {available_models()}\")\n",
    "print(f\"Selected model  : {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9c31a",
   "metadata": {},
   "source": [
    "## 14. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "# Build config\n",
    "cfg = TrainConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    freeze_backbone=FREEZE_BACKBONE,\n",
    "    dropout=DROPOUT,\n",
    "    manifest=paths[\"manifest_clean\"],\n",
    "    label_map=paths[\"label_map\"],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler=LR_SCHEDULER,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    seed=SEED,\n",
    "    device=None,  # auto-detect (will use GPU if available)\n",
    ")\n",
    "\n",
    "# Build datasets\n",
    "print(\"Loading datasets …\")\n",
    "datasets = build_datasets(\n",
    "    manifest_path=cfg.manifest,\n",
    "    label_map_path=cfg.label_map,\n",
    "    train_transform=get_train_transforms(size=cfg.image_size),\n",
    "    val_transform=get_val_transforms(size=cfg.image_size),\n",
    ")\n",
    "\n",
    "train_ds = datasets[\"train\"]\n",
    "val_ds = datasets[\"val\"]\n",
    "\n",
    "print(f\"  train : {train_ds}\")\n",
    "print(f\"  val   : {val_ds}\")\n",
    "\n",
    "class_names = train_ds.classes\n",
    "num_classes = train_ds.num_classes\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Build model\n",
    "print(f\"Building model: {cfg.model_name} …\")\n",
    "model = build_model(\n",
    "    name=cfg.model_name,\n",
    "    num_classes=num_classes,\n",
    "    freeze_backbone=cfg.freeze_backbone,\n",
    "    dropout=cfg.dropout,\n",
    ")\n",
    "\n",
    "if hasattr(model, \"param_summary\"):\n",
    "    ps = model.param_summary()\n",
    "    print(f\"  params  total={ps['total']:,}  trainable={ps['trainable']:,}  frozen={ps['frozen']:,}\")\n",
    "\n",
    "# Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    cfg=cfg,\n",
    "    class_names=class_names,\n",
    ")\n",
    "\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057c9f3",
   "metadata": {},
   "source": [
    "## 15. Display Training Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "run_dir = cfg.run_dir()\n",
    "\n",
    "for plot_name in [\"loss_curve.png\", \"accuracy_curve.png\", \"confusion_matrix.png\"]:\n",
    "    plot_path = run_dir / plot_name\n",
    "    if plot_path.exists():\n",
    "        print(f\"\\n{plot_name}:\")\n",
    "        display(IPImage(filename=str(plot_path)))\n",
    "    else:\n",
    "        print(f\"⚠️  {plot_name} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183c9f9",
   "metadata": {},
   "source": [
    "## 16. (Optional) Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad589db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = datasets[\"test\"]\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Load best checkpoint\n",
    "device = trainer.device\n",
    "best_path = trainer.best_ckpt_path\n",
    "\n",
    "if best_path.exists():\n",
    "    print(f\"Loading best checkpoint: {best_path}\")\n",
    "    ckpt = torch.load(best_path, map_location=str(device))\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Test\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            logits = model(images)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "    test_metrics = compute_metrics(all_labels, all_preds, class_names)\n",
    "    print(f\"\\n── Test Results ──\")\n",
    "    print(f\"  Accuracy : {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Macro : {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    print(f\"\\n── Test Classification Report ──\")\n",
    "    print(get_classification_report(all_labels, all_preds, class_names))\n",
    "\n",
    "    cm = get_confusion_matrix(all_labels, all_preds)\n",
    "    plot_confusion_matrix(\n",
    "        cm, class_names,\n",
    "        run_dir / \"test_confusion_matrix.png\",\n",
    "        title=f\"Test Confusion Matrix — {cfg.model_name}\",\n",
    "    )\n",
    "    display(IPImage(filename=str(run_dir / \"test_confusion_matrix.png\")))\n",
    "else:\n",
    "    print(\"No checkpoint found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1fea7",
   "metadata": {},
   "source": [
    "## 17. (Optional) Download Model & Results\n",
    "\n",
    "Uncomment to download outputs to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd016f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# \n",
    "# # Download best checkpoint\n",
    "# if trainer.best_ckpt_path.exists():\n",
    "#     files.download(str(trainer.best_ckpt_path))\n",
    "# \n",
    "# # Download metrics CSV\n",
    "# csv_path = cfg.run_dir() / \"metrics.csv\"\n",
    "# if csv_path.exists():\n",
    "#     files.download(str(csv_path))\n",
    "# \n",
    "# # Or zip the entire outputs folder\n",
    "# !zip -r outputs.zip outputs/\n",
    "# files.download(\"outputs.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}